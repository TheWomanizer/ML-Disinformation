{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-intro",
      "metadata": {},
      "source": [
        "# TruthSeeker: Ensemble Completo para Detección de Desinformación\n",
        "\n",
        "En este notebook implemento el sistema TruthSeeker, un ensemble híbrido que combina múltiples arquitecturas de machine learning y deep learning para la detección avanzada de desinformación. El sistema integra modelos tradicionales, algoritmos de boosting, redes neuronales, modelos NLP y transformers BERT en un meta-ensemble robusto.\n",
        "\n",
        "## Arquitectura TruthSeeker\n",
        "1. **Nivel Base**: Modelos individuales especializados\n",
        "2. **Nivel Meta**: Ensemble learning con votación ponderada\n",
        "3. **Nivel Final**: Meta-modelo para decisión final\n",
        "4. **Sistema de Confianza**: Métricas de certeza para cada predicción\n",
        "\n",
        "## Objetivos\n",
        "- Maximizo la precisión mediante ensemble diversity\n",
        "- Implemento sistema de confianza para evaluación de predicciones\n",
        "- Genero visualizaciones interactivas completas con Plotly\n",
        "- Produzco análisis exhaustivo de rendimiento y características del modelo\n",
        "- Creo dashboard interactivo para evaluación en tiempo real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejecutando en entorno local\n",
            "Transformers disponible para integración BERT\n",
            "TruthSeeker Ensemble System Iniciado\n",
            "Timestamp: 2025-08-24 21:19:39\n",
            "PyTorch: 2.8.0+cpu\n",
            "BERT disponible: True\n"
          ]
        }
      ],
      "source": [
        "# Detección del entorno de ejecución\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Configuración del entorno e importaciones principales\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "import gc\n",
        "\n",
        "# Instalación de dependencias para Colab\n",
        "if IN_COLAB:\n",
        "    !pip install plotly xgboost lightgbm catboost transformers torch scikit-learn==1.3.2 -q\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Ejecutando en Google Colab\")\n",
        "else:\n",
        "    print(\"Ejecutando en entorno local\")\n",
        "\n",
        "# Visualización avanzada con Plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.colors import qualitative, sequential, diverging\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
        "    classification_report, confusion_matrix, roc_curve, precision_recall_curve,\n",
        "    average_precision_score, matthews_corrcoef, cohen_kappa_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "\n",
        "# Deep Learning\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "\n",
        "# Transformers para BERT\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    BERT_AVAILABLE = True\n",
        "    print(\"Transformers disponible para integración BERT\")\n",
        "except ImportError:\n",
        "    BERT_AVAILABLE = False\n",
        "    print(\"Transformers no disponible - ensemble sin BERT\")\n",
        "\n",
        "# Utilidades\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configuración de estilo para visualizaciones\n",
        "plt.style.use('default')\n",
        "PLOTLY_THEME = 'plotly_white'\n",
        "COLORS = px.colors.qualitative.Set3\n",
        "SEQUENTIAL_COLORS = px.colors.sequential.Viridis\n",
        "\n",
        "print(f\"TruthSeeker Ensemble System Iniciado\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "if TORCH_AVAILABLE:\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"BERT disponible: {BERT_AVAILABLE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-config",
      "metadata": {},
      "source": [
        "## Configuración del Sistema TruthSeeker\n",
        "\n",
        "Configuro las rutas, parámetros y componentes principales del sistema ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuración TruthSeeker:\n",
            "  cross_validation_folds: 5\n",
            "  confidence_threshold: 0.7\n",
            "  voting_weights: auto\n",
            "  calibration_method: isotonic\n",
            "  meta_model: xgboost\n",
            "  diversity_bonus: 0.1\n",
            "  min_model_performance: 0.6\n",
            "  use_bert: True\n",
            "  use_nlp: True\n",
            "  use_traditional: True\n",
            "  use_boosting: True\n",
            "  use_neural_networks: True\n",
            "\n",
            "Rutas configuradas:\n",
            "  Modelos: ..\\models\n",
            "  Datos: ..\\processed_data\n",
            "  Resultados: ..\\results\\truthseeker_ensemble\n",
            "  BERT: ..\\models\\bert_models\n",
            "  NLP: ..\\models\\nlp_models\n"
          ]
        }
      ],
      "source": [
        "# Configuración de rutas y parámetros\n",
        "if IN_COLAB:\n",
        "    BASE_PATH = Path('/content')\n",
        "    MODELS_PATH = BASE_PATH / 'models'\n",
        "    DATA_PATH = BASE_PATH / 'processed_data'\n",
        "else:\n",
        "    BASE_PATH = Path('../')\n",
        "    MODELS_PATH = BASE_PATH / 'models'\n",
        "    DATA_PATH = BASE_PATH / 'processed_data'\n",
        "\n",
        "RESULTS_PATH = BASE_PATH / 'results' / 'truthseeker_ensemble'\n",
        "BERT_MODELS_PATH = MODELS_PATH / 'bert_models'\n",
        "NLP_MODELS_PATH = MODELS_PATH / 'nlp_models'\n",
        "\n",
        "# Creo directorios necesarios\n",
        "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
        "(RESULTS_PATH / 'visualizations').mkdir(exist_ok=True)\n",
        "(RESULTS_PATH / 'models').mkdir(exist_ok=True)\n",
        "\n",
        "# Configuración del ensemble\n",
        "ENSEMBLE_CONFIG = {\n",
        "    'cross_validation_folds': 5,\n",
        "    'confidence_threshold': 0.7,\n",
        "    'voting_weights': 'auto',  # Se calculan dinámicamente\n",
        "    'calibration_method': 'isotonic',\n",
        "    'meta_model': 'xgboost',  # Meta-learner final\n",
        "    'diversity_bonus': 0.1,  # Bonus para modelos diversos\n",
        "    'min_model_performance': 0.6,  # F1 mínimo para incluir modelo\n",
        "    'use_bert': BERT_AVAILABLE,\n",
        "    'use_nlp': True,\n",
        "    'use_traditional': True,\n",
        "    'use_boosting': True,\n",
        "    'use_neural_networks': True\n",
        "}\n",
        "\n",
        "# Configuración de visualizaciones\n",
        "VIZ_CONFIG = {\n",
        "    'height': 600,\n",
        "    'width': 1000,\n",
        "    'template': PLOTLY_THEME,\n",
        "    'color_palette': COLORS,\n",
        "    'font_size': 12,\n",
        "    'title_font_size': 16,\n",
        "    'save_html': True,\n",
        "    'save_png': True,\n",
        "    'interactive': True\n",
        "}\n",
        "\n",
        "print(f\"Configuración TruthSeeker:\")\n",
        "for key, value in ENSEMBLE_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nRutas configuradas:\")\n",
        "print(f\"  Modelos: {MODELS_PATH}\")\n",
        "print(f\"  Datos: {DATA_PATH}\")\n",
        "print(f\"  Resultados: {RESULTS_PATH}\")\n",
        "print(f\"  BERT: {BERT_MODELS_PATH}\")\n",
        "print(f\"  NLP: {NLP_MODELS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-data-load",
      "metadata": {},
      "source": [
        "## Carga de Datos y Modelos Base\n",
        "\n",
        "Cargo los datos procesados y todos los modelos entrenados previamente para integrarlos en el ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-load-data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos procesados no encontrados - cargando dataset original\n",
            "Dataset original cargado: (134198, 9)\n",
            "Creando features sintéticas para demo\n",
            "Datos procesados y guardados:\n",
            "  Features: ['feature_1', 'feature_2', 'feature_3']\n",
            "  Train: (107358, 3)\n",
            "  Test: (26840, 3)\n",
            "\n",
            "Distribución de etiquetas:\n",
            "Train: [43187 64171] (proportions: [0.40227091 0.59772909])\n",
            "Test: [10797 16043] (proportions: [0.40227273 0.59772727])\n"
          ]
        }
      ],
      "source": [
        "# Función para cargar datos inteligentemente\n",
        "def load_processed_data():\n",
        "    \"\"\"Carga los datos procesados del sistema\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Intento cargar datos procesados\n",
        "        X_train = joblib.load(DATA_PATH / 'X_train.pkl')\n",
        "        X_test = joblib.load(DATA_PATH / 'X_test.pkl') \n",
        "        y_train = joblib.load(DATA_PATH / 'y_train.pkl')\n",
        "        y_test = joblib.load(DATA_PATH / 'y_test.pkl')\n",
        "        \n",
        "        print(f\"Datos procesados cargados:\")\n",
        "        print(f\"  Train: {X_train.shape} features, {len(y_train)} samples\")\n",
        "        print(f\"  Test: {X_test.shape} features, {len(y_test)} samples\")\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(\"Datos procesados no encontrados - cargando dataset original\")\n",
        "        \n",
        "        # Cargo dataset original como fallback\n",
        "        if IN_COLAB:\n",
        "            dataset_path = BASE_PATH / 'Truth_Seeker_Model_Dataset.csv'\n",
        "        else:\n",
        "            dataset_path = BASE_PATH / 'dataset1' / 'Truth_Seeker_Model_Dataset.csv'\n",
        "        \n",
        "        if dataset_path.exists():\n",
        "            df = pd.read_csv(dataset_path)\n",
        "            print(f\"Dataset original cargado: {df.shape}\")\n",
        "            \n",
        "            # Preproceso básico para el ensemble\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "            \n",
        "            # Selecciono features numéricas relevantes\n",
        "            numeric_features = ['Age', 'Hours_Spent', 'Followers_Count', 'Posts_Per_Day']\n",
        "            available_features = [f for f in numeric_features if f in df.columns]\n",
        "            \n",
        "            if len(available_features) == 0:\n",
        "                print(\"Creando features sintéticas para demo\")\n",
        "                # Creo features sintéticas si no hay numéricas disponibles\n",
        "                df['feature_1'] = np.random.randn(len(df))\n",
        "                df['feature_2'] = np.random.randn(len(df))\n",
        "                df['feature_3'] = np.random.randn(len(df))\n",
        "                available_features = ['feature_1', 'feature_2', 'feature_3']\n",
        "            \n",
        "            X = df[available_features].fillna(0)\n",
        "            \n",
        "            # Preparo etiquetas\n",
        "            if 'Believed_Misinformation' in df.columns:\n",
        "                # Mapeo Believed_Misinformation a binario\n",
        "                label_map = {'No': 0, 'Yes': 1, 'Maybe': 0}\n",
        "                y = df['Believed_Misinformation'].map(label_map).fillna(0)\n",
        "            else:\n",
        "                # Etiquetas sintéticas\n",
        "                y = np.random.choice([0, 1], len(df), p=[0.4, 0.6])\n",
        "            \n",
        "            # División train/test\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42, stratify=y\n",
        "            )\n",
        "            \n",
        "            # Escalado\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "            \n",
        "            # Guardo para futuro uso\n",
        "            joblib.dump(X_train, DATA_PATH / 'X_train.pkl')\n",
        "            joblib.dump(X_test, DATA_PATH / 'X_test.pkl')\n",
        "            joblib.dump(y_train, DATA_PATH / 'y_train.pkl')\n",
        "            joblib.dump(y_test, DATA_PATH / 'y_test.pkl')\n",
        "            joblib.dump(scaler, DATA_PATH / 'scaler.pkl')\n",
        "            \n",
        "            print(f\"Datos procesados y guardados:\")\n",
        "            print(f\"  Features: {available_features}\")\n",
        "            print(f\"  Train: {X_train.shape}\")\n",
        "            print(f\"  Test: {X_test.shape}\")\n",
        "            \n",
        "            return X_train, X_test, y_train, y_test\n",
        "            \n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No se encontró dataset en {dataset_path}\")\n",
        "\n",
        "# Cargo los datos\n",
        "X_train, X_test, y_train, y_test = load_processed_data()\n",
        "\n",
        "print(f\"\\nDistribución de etiquetas:\")\n",
        "print(f\"Train: {np.bincount(y_train)} (proportions: {np.bincount(y_train) / len(y_train)})\")\n",
        "print(f\"Test: {np.bincount(y_test)} (proportions: {np.bincount(y_test) / len(y_test)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-load-models",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando carga de modelos...\n",
            "Modelo NLP cargado: nlp_logistic_regression_tfidf\n",
            "Modelo NLP cargado: nlp_svm_linear_tfidf\n",
            "Modelo NLP cargado: nlp_tfidf_vectorizer\n",
            "\n",
            "Resumen de modelos cargados:\n",
            "  Nlp: 3 modelos\n",
            "Total: 3 modelos cargados\n",
            "\n",
            "Sistema listo con 3 modelos para ensemble\n"
          ]
        }
      ],
      "source": [
        "# Cargador inteligente de modelos\n",
        "class ModelLoader:\n",
        "    \"\"\"Clase para cargar modelos de diferentes tipos de manera robusta\"\"\"\n",
        "    \n",
        "    def __init__(self, models_path):\n",
        "        self.models_path = Path(models_path)\n",
        "        self.loaded_models = {}\n",
        "        self.model_performance = {}\n",
        "        self.model_types = {\n",
        "            'traditional': [],\n",
        "            'boosting': [],\n",
        "            'neural': [],\n",
        "            'nlp': [],\n",
        "            'bert': []\n",
        "        }\n",
        "    \n",
        "    def load_traditional_models(self):\n",
        "        \"\"\"Carga modelos tradicionales (SVM, RF, etc.)\"\"\"\n",
        "        traditional_patterns = ['*.pkl', '*traditional*.joblib', '*svm*.pkl', '*rf*.pkl']\n",
        "        \n",
        "        for pattern in traditional_patterns:\n",
        "            for model_file in self.models_path.glob(pattern):\n",
        "                if 'traditional' in model_file.name.lower():\n",
        "                    try:\n",
        "                        model = joblib.load(model_file)\n",
        "                        model_name = model_file.stem\n",
        "                        self.loaded_models[model_name] = model\n",
        "                        self.model_types['traditional'].append(model_name)\n",
        "                        print(f\"Modelo tradicional cargado: {model_name}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error cargando {model_file.name}: {e}\")\n",
        "    \n",
        "    def load_boosting_models(self):\n",
        "        \"\"\"Carga modelos de boosting\"\"\"\n",
        "        boosting_patterns = ['*xgb*.pkl', '*lgb*.pkl', '*catboost*.pkl', '*boosting*.pkl']\n",
        "        \n",
        "        for pattern in boosting_patterns:\n",
        "            for model_file in self.models_path.glob(pattern):\n",
        "                try:\n",
        "                    model = joblib.load(model_file)\n",
        "                    model_name = model_file.stem\n",
        "                    self.loaded_models[model_name] = model\n",
        "                    self.model_types['boosting'].append(model_name)\n",
        "                    print(f\"Modelo boosting cargado: {model_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error cargando {model_file.name}: {e}\")\n",
        "    \n",
        "    def load_neural_models(self):\n",
        "        \"\"\"Carga redes neuronales (PyTorch, TensorFlow)\"\"\"\n",
        "        neural_patterns = ['*.pth', '*.pt', '*neural*.pkl', '*nn*.pkl']\n",
        "        \n",
        "        for pattern in neural_patterns:\n",
        "            for model_file in self.models_path.glob(pattern):\n",
        "                try:\n",
        "                    if model_file.suffix in ['.pth', '.pt']:\n",
        "                        # Modelo PyTorch - necesitaría la arquitectura\n",
        "                        print(f\"Modelo PyTorch encontrado pero requiere arquitectura: {model_file.name}\")\n",
        "                        continue\n",
        "                    else:\n",
        "                        model = joblib.load(model_file)\n",
        "                        model_name = model_file.stem\n",
        "                        self.loaded_models[model_name] = model\n",
        "                        self.model_types['neural'].append(model_name)\n",
        "                        print(f\"Modelo neural cargado: {model_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error cargando {model_file.name}: {e}\")\n",
        "    \n",
        "    def load_bert_models(self):\n",
        "        \"\"\"Carga modelos BERT\"\"\"\n",
        "        if not BERT_AVAILABLE:\n",
        "            print(\"BERT no disponible - saltando carga de modelos BERT\")\n",
        "            return\n",
        "        \n",
        "        bert_path = self.models_path / 'bert_models'\n",
        "        if bert_path.exists():\n",
        "            for bert_dir in bert_path.iterdir():\n",
        "                if bert_dir.is_dir() and (bert_dir / 'config.json').exists():\n",
        "                    try:\n",
        "                        tokenizer = AutoTokenizer.from_pretrained(bert_dir)\n",
        "                        model = AutoModelForSequenceClassification.from_pretrained(bert_dir)\n",
        "                        \n",
        "                        model_name = f\"bert_{bert_dir.name}\"\n",
        "                        self.loaded_models[model_name] = {'model': model, 'tokenizer': tokenizer}\n",
        "                        self.model_types['bert'].append(model_name)\n",
        "                        print(f\"Modelo BERT cargado: {model_name}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error cargando BERT {bert_dir.name}: {e}\")\n",
        "    \n",
        "    def load_nlp_models(self):\n",
        "        \"\"\"Carga modelos NLP (TF-IDF, etc.)\"\"\"\n",
        "        nlp_path = self.models_path / 'nlp_models'\n",
        "        if nlp_path.exists():\n",
        "            for nlp_file in nlp_path.glob('*.pkl'):\n",
        "                try:\n",
        "                    model = joblib.load(nlp_file)\n",
        "                    model_name = f\"nlp_{nlp_file.stem}\"\n",
        "                    self.loaded_models[model_name] = model\n",
        "                    self.model_types['nlp'].append(model_name)\n",
        "                    print(f\"Modelo NLP cargado: {model_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error cargando NLP {nlp_file.name}: {e}\")\n",
        "    \n",
        "    def load_all_models(self):\n",
        "        \"\"\"Carga todos los tipos de modelos\"\"\"\n",
        "        print(\"Iniciando carga de modelos...\")\n",
        "        \n",
        "        if ENSEMBLE_CONFIG['use_traditional']:\n",
        "            self.load_traditional_models()\n",
        "        \n",
        "        if ENSEMBLE_CONFIG['use_boosting']:\n",
        "            self.load_boosting_models()\n",
        "        \n",
        "        if ENSEMBLE_CONFIG['use_neural_networks']:\n",
        "            self.load_neural_models()\n",
        "        \n",
        "        if ENSEMBLE_CONFIG['use_nlp']:\n",
        "            self.load_nlp_models()\n",
        "        \n",
        "        if ENSEMBLE_CONFIG['use_bert']:\n",
        "            self.load_bert_models()\n",
        "        \n",
        "        print(f\"\\nResumen de modelos cargados:\")\n",
        "        total_models = 0\n",
        "        for model_type, models in self.model_types.items():\n",
        "            count = len(models)\n",
        "            total_models += count\n",
        "            if count > 0:\n",
        "                print(f\"  {model_type.capitalize()}: {count} modelos\")\n",
        "        \n",
        "        print(f\"Total: {total_models} modelos cargados\")\n",
        "        return total_models\n",
        "\n",
        "# Cargo todos los modelos\n",
        "model_loader = ModelLoader(MODELS_PATH)\n",
        "total_loaded = model_loader.load_all_models()\n",
        "\n",
        "if total_loaded == 0:\n",
        "    print(\"\\nNo se cargaron modelos - creando modelos demo para ensemble\")\n",
        "    \n",
        "    # Creo modelos simples para demo\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "    demo_models = {\n",
        "        'rf_demo': RandomForestClassifier(n_estimators=50, random_state=42),\n",
        "        'svm_demo': SVC(probability=True, random_state=42),\n",
        "        'lr_demo': LogisticRegression(random_state=42)\n",
        "    }\n",
        "    \n",
        "    for name, model in demo_models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        model_loader.loaded_models[name] = model\n",
        "        model_loader.model_types['traditional'].append(name)\n",
        "    \n",
        "    print(f\"{len(demo_models)} modelos demo creados y entrenados\")\n",
        "    total_loaded = len(demo_models)\n",
        "\n",
        "print(f\"\\nSistema listo con {total_loaded} modelos para ensemble\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-evaluation",
      "metadata": {},
      "source": [
        "## Evaluación Individual de Modelos Base\n",
        "\n",
        "Evalúo cada modelo individualmente para determinar pesos y seleccionar los mejores para el ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-evaluate-models",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluando modelos individuales...\n",
            "Error evaluando nlp_logistic_regression_tfidf: X has 3 features, but LogisticRegression is expecting 10000 features as input.\n",
            "Error evaluando nlp_svm_linear_tfidf: X has 3 features, but LinearSVC is expecting 10000 features as input.\n",
            "Error evaluando nlp_tfidf_vectorizer: 'TfidfVectorizer' object has no attribute 'predict'\n",
            "\n",
            "Evaluación completada: 0 modelos evaluados\n",
            "No se pudieron evaluar modelos\n"
          ]
        }
      ],
      "source": [
        "# Evaluador de modelos individuales\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Evalúa modelos individuales y calcula métricas de rendimiento\"\"\"\n",
        "    \n",
        "    def __init__(self, X_train, X_test, y_train, y_test):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.results = {}\n",
        "        self.predictions = {}\n",
        "        self.probabilities = {}\n",
        "    \n",
        "    def evaluate_single_model(self, name, model, model_type='traditional'):\n",
        "        \"\"\"Evalúa un modelo individual\"\"\"\n",
        "        \n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            if model_type == 'bert':\n",
        "                # Manejo especial para BERT - requeriría texto\n",
        "                print(f\"Evaluación BERT requiere datos de texto - saltando {name}\")\n",
        "                return None\n",
        "            \n",
        "            # Predicciones\n",
        "            y_pred = model.predict(self.X_test)\n",
        "            \n",
        "            # Probabilidades\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                y_prob = model.predict_proba(self.X_test)[:, 1]\n",
        "            elif hasattr(model, 'decision_function'):\n",
        "                from sklearn.preprocessing import MinMaxScaler\n",
        "                decision_scores = model.decision_function(self.X_test)\n",
        "                scaler = MinMaxScaler()\n",
        "                y_prob = scaler.fit_transform(decision_scores.reshape(-1, 1)).flatten()\n",
        "            else:\n",
        "                y_prob = y_pred.astype(float)\n",
        "            \n",
        "            # Calculo métricas\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                self.y_test, y_pred, average='binary', zero_division=0\n",
        "            )\n",
        "            \n",
        "            try:\n",
        "                roc_auc = roc_auc_score(self.y_test, y_prob)\n",
        "                pr_auc = average_precision_score(self.y_test, y_prob)\n",
        "            except:\n",
        "                roc_auc = pr_auc = 0.0\n",
        "            \n",
        "            mcc = matthews_corrcoef(self.y_test, y_pred)\n",
        "            kappa = cohen_kappa_score(self.y_test, y_pred)\n",
        "            \n",
        "            # Cross-validation en train set\n",
        "            try:\n",
        "                cv_scores = cross_val_score(\n",
        "                    model, self.X_train, self.y_train, \n",
        "                    cv=3, scoring='f1', n_jobs=-1\n",
        "                )\n",
        "                cv_mean = cv_scores.mean()\n",
        "                cv_std = cv_scores.std()\n",
        "            except:\n",
        "                cv_mean = cv_std = 0.0\n",
        "            \n",
        "            inference_time = time.time() - start_time\n",
        "            \n",
        "            results = {\n",
        "                'model_name': name,\n",
        "                'model_type': model_type,\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1,\n",
        "                'roc_auc': roc_auc,\n",
        "                'pr_auc': pr_auc,\n",
        "                'matthews_corr': mcc,\n",
        "                'cohen_kappa': kappa,\n",
        "                'cv_f1_mean': cv_mean,\n",
        "                'cv_f1_std': cv_std,\n",
        "                'inference_time': inference_time,\n",
        "                'predictions': y_pred,\n",
        "                'probabilities': y_prob\n",
        "            }\n",
        "            \n",
        "            # Guardo resultados\n",
        "            self.results[name] = results\n",
        "            self.predictions[name] = y_pred\n",
        "            self.probabilities[name] = y_prob\n",
        "            \n",
        "            print(f\"{name}: F1={f1:.3f}, ROC-AUC={roc_auc:.3f}, ACC={accuracy:.3f}\")\n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluando {name}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def evaluate_all_models(self, model_loader):\n",
        "        \"\"\"Evalúa todos los modelos cargados\"\"\"\n",
        "        print(\"Evaluando modelos individuales...\")\n",
        "        \n",
        "        successful_evaluations = 0\n",
        "        \n",
        "        for name, model in model_loader.loaded_models.items():\n",
        "            # Determino tipo de modelo\n",
        "            model_type = 'traditional'\n",
        "            for mtype, models in model_loader.model_types.items():\n",
        "                if name in models:\n",
        "                    model_type = mtype\n",
        "                    break\n",
        "            \n",
        "            result = self.evaluate_single_model(name, model, model_type)\n",
        "            if result is not None:\n",
        "                successful_evaluations += 1\n",
        "        \n",
        "        print(f\"\\nEvaluación completada: {successful_evaluations} modelos evaluados\")\n",
        "        return successful_evaluations\n",
        "    \n",
        "    def get_results_dataframe(self):\n",
        "        \"\"\"Convierte resultados a DataFrame para análisis\"\"\"\n",
        "        if not self.results:\n",
        "            return None\n",
        "        \n",
        "        df_data = []\n",
        "        for name, result in self.results.items():\n",
        "            row = {k: v for k, v in result.items() \n",
        "                   if k not in ['predictions', 'probabilities']}\n",
        "            df_data.append(row)\n",
        "        \n",
        "        df = pd.DataFrame(df_data)\n",
        "        df = df.sort_values('f1_score', ascending=False)\n",
        "        return df\n",
        "\n",
        "# Evalúo todos los modelos\n",
        "evaluator = ModelEvaluator(X_train, X_test, y_train, y_test)\n",
        "num_evaluated = evaluator.evaluate_all_models(model_loader)\n",
        "\n",
        "# Obtengo resultados\n",
        "results_df = evaluator.get_results_dataframe()\n",
        "\n",
        "if results_df is not None:\n",
        "    print(f\"\\nRESUMEN DE RENDIMIENTO INDIVIDUAL:\")\n",
        "    print(results_df[['model_name', 'model_type', 'f1_score', 'roc_auc', 'accuracy']].round(3))\n",
        "    \n",
        "    # Filtro modelos de alto rendimiento\n",
        "    good_models = results_df[results_df['f1_score'] >= ENSEMBLE_CONFIG['min_model_performance']]\n",
        "    print(f\"\\nModelos que califican para ensemble (F1 >= {ENSEMBLE_CONFIG['min_model_performance']}): {len(good_models)}\")\n",
        "    \n",
        "    if len(good_models) > 0:\n",
        "        print(good_models[['model_name', 'f1_score', 'roc_auc']].round(3).to_string(index=False))\n",
        "    else:\n",
        "        print(\"Ningún modelo alcanza el rendimiento mínimo - usando todos los disponibles\")\n",
        "        good_models = results_df\n",
        "else:\n",
        "    print(\"No se pudieron evaluar modelos\")\n",
        "    good_models = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-viz-individual",
      "metadata": {},
      "source": [
        "## Visualizaciones de Rendimiento Individual\n",
        "\n",
        "Creo visualizaciones interactivas detalladas del rendimiento de cada modelo usando Plotly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell-viz-performance",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay datos para crear visualizaciones\n"
          ]
        }
      ],
      "source": [
        "# Visualizador de rendimiento con Plotly avanzado\n",
        "class PerformanceVisualizer:\n",
        "    \"\"\"Crea visualizaciones avanzadas de rendimiento con Plotly\"\"\"\n",
        "    \n",
        "    def __init__(self, results_df, evaluator, viz_config):\n",
        "        self.results_df = results_df\n",
        "        self.evaluator = evaluator\n",
        "        self.config = viz_config\n",
        "        self.colors = px.colors.qualitative.Set3\n",
        "        \n",
        "    def create_performance_dashboard(self):\n",
        "        \"\"\"Crea dashboard completo de rendimiento\"\"\"\n",
        "        \n",
        "        if self.results_df is None or len(self.results_df) == 0:\n",
        "            print(\"No hay datos para visualizar\")\n",
        "            return None\n",
        "        \n",
        "        # Dashboard con múltiples subplots\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=[\n",
        "                'F1-Score por Modelo', 'ROC-AUC vs Precisión',\n",
        "                'Métricas Múltiples', 'Tiempo de Inferencia',\n",
        "                'Validación Cruzada', 'Matriz de Correlación de Métricas'\n",
        "            ],\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"heatmap\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.08,\n",
        "            horizontal_spacing=0.1\n",
        "        )\n",
        "        \n",
        "        # 1. F1-Score por modelo\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=self.results_df['model_name'],\n",
        "                y=self.results_df['f1_score'],\n",
        "                name='F1-Score',\n",
        "                text=[f'{f:.3f}' for f in self.results_df['f1_score']],\n",
        "                textposition='outside',\n",
        "                marker_color=self.colors[0],\n",
        "                hovertemplate='<b>%{x}</b><br>F1-Score: %{y:.3f}<extra></extra>'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. ROC-AUC vs Precisión\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=self.results_df['roc_auc'],\n",
        "                y=self.results_df['precision'],\n",
        "                mode='markers+text',\n",
        "                text=self.results_df['model_name'],\n",
        "                textposition='top center',\n",
        "                name='ROC-AUC vs Precisión',\n",
        "                marker=dict(\n",
        "                    size=self.results_df['f1_score'] * 20,\n",
        "                    color=self.results_df['f1_score'],\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"F1-Score\")\n",
        "                ),\n",
        "                hovertemplate='<b>%{text}</b><br>ROC-AUC: %{x:.3f}<br>Precisión: %{y:.3f}<extra></extra>'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Métricas múltiples (Radar-style en barras)\n",
        "        metrics = ['f1_score', 'roc_auc', 'precision', 'recall']\n",
        "        best_model_idx = self.results_df['f1_score'].idxmax()\n",
        "        best_model = self.results_df.loc[best_model_idx]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=metrics,\n",
        "                y=[best_model[m] for m in metrics],\n",
        "                name=f'Mejor Modelo: {best_model[\"model_name\"]}',\n",
        "                text=[f'{best_model[m]:.3f}' for m in metrics],\n",
        "                textposition='outside',\n",
        "                marker_color=self.colors[1]\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Tiempo de inferencia\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=self.results_df['model_name'],\n",
        "                y=self.results_df['inference_time'],\n",
        "                name='Tiempo Inferencia',\n",
        "                text=[f'{t:.3f}s' for t in self.results_df['inference_time']],\n",
        "                textposition='outside',\n",
        "                marker_color=self.colors[2]\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # 5. Validación cruzada\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=self.results_df['model_name'],\n",
        "                y=self.results_df['cv_f1_mean'],\n",
        "                error_y=dict(type='data', array=self.results_df['cv_f1_std']),\n",
        "                name='CV F1-Score',\n",
        "                text=[f'{cv:.3f}±{std:.3f}' for cv, std in \n",
        "                     zip(self.results_df['cv_f1_mean'], self.results_df['cv_f1_std'])],\n",
        "                textposition='outside',\n",
        "                marker_color=self.colors[3]\n",
        "            ),\n",
        "            row=3, col=1\n",
        "        )\n",
        "        \n",
        "        # 6. Matriz de correlación de métricas\n",
        "        metrics_for_corr = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "        corr_matrix = self.results_df[metrics_for_corr].corr()\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Heatmap(\n",
        "                z=corr_matrix.values,\n",
        "                x=corr_matrix.columns,\n",
        "                y=corr_matrix.columns,\n",
        "                colorscale='RdBu',\n",
        "                zmid=0,\n",
        "                text=corr_matrix.round(2).values,\n",
        "                texttemplate='%{text}',\n",
        "                textfont={\"size\": 10},\n",
        "                name='Correlación Métricas'\n",
        "            ),\n",
        "            row=3, col=2\n",
        "        )\n",
        "        \n",
        "        # Configuración del layout\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': 'Dashboard de Rendimiento - Modelos Individuales TruthSeeker',\n",
        "                'x': 0.5,\n",
        "                'font': {'size': 20}\n",
        "            },\n",
        "            height=1200,\n",
        "            showlegend=False,\n",
        "            template=self.config['template']\n",
        "        )\n",
        "        \n",
        "        # Actualizo ejes\n",
        "        fig.update_xaxes(tickangle=45, row=1, col=1)\n",
        "        fig.update_xaxes(tickangle=45, row=2, col=2)\n",
        "        fig.update_xaxes(tickangle=45, row=3, col=1)\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    def create_roc_curves_comparison(self):\n",
        "        \"\"\"Crea comparación de curvas ROC\"\"\"\n",
        "        \n",
        "        fig = go.Figure()\n",
        "        \n",
        "        # Línea diagonal de referencia\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[0, 1], y=[0, 1],\n",
        "                mode='lines',\n",
        "                name='Aleatorio (AUC = 0.5)',\n",
        "                line=dict(dash='dash', color='gray')\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # ROC para cada modelo\n",
        "        colors = px.colors.qualitative.Set1\n",
        "        for i, (name, result) in enumerate(self.evaluator.results.items()):\n",
        "            if 'probabilities' in result:\n",
        "                try:\n",
        "                    fpr, tpr, _ = roc_curve(self.evaluator.y_test, result['probabilities'])\n",
        "                    auc = result['roc_auc']\n",
        "                    \n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=fpr, y=tpr,\n",
        "                            mode='lines',\n",
        "                            name=f'{name} (AUC = {auc:.3f})',\n",
        "                            line=dict(color=colors[i % len(colors)]),\n",
        "                            hovertemplate='<b>%{fullData.name}</b><br>FPR: %{x:.3f}<br>TPR: %{y:.3f}<extra></extra>'\n",
        "                        )\n",
        "                    )\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title='Comparación de Curvas ROC - TruthSeeker Models',\n",
        "            xaxis_title='Tasa de Falsos Positivos (FPR)',\n",
        "            yaxis_title='Tasa de Verdaderos Positivos (TPR)',\n",
        "            template=self.config['template'],\n",
        "            height=600,\n",
        "            legend=dict(x=0.6, y=0.1)\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    def create_precision_recall_curves(self):\n",
        "        \"\"\"Crea curvas Precisión-Recall\"\"\"\n",
        "        \n",
        "        fig = go.Figure()\n",
        "        \n",
        "        # Línea de referencia (baseline)\n",
        "        baseline = np.mean(self.evaluator.y_test)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[0, 1], y=[baseline, baseline],\n",
        "                mode='lines',\n",
        "                name=f'Baseline (AP = {baseline:.3f})',\n",
        "                line=dict(dash='dash', color='gray')\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # PR curve para cada modelo\n",
        "        colors = px.colors.qualitative.Set2\n",
        "        for i, (name, result) in enumerate(self.evaluator.results.items()):\n",
        "            if 'probabilities' in result:\n",
        "                try:\n",
        "                    precision, recall, _ = precision_recall_curve(\n",
        "                        self.evaluator.y_test, result['probabilities']\n",
        "                    )\n",
        "                    ap = result['pr_auc']\n",
        "                    \n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=recall, y=precision,\n",
        "                            mode='lines',\n",
        "                            name=f'{name} (AP = {ap:.3f})',\n",
        "                            line=dict(color=colors[i % len(colors)])\n",
        "                        )\n",
        "                    )\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title='Curvas Precisión-Recall - TruthSeeker Models',\n",
        "            xaxis_title='Recall',\n",
        "            yaxis_title='Precisión',\n",
        "            template=self.config['template'],\n",
        "            height=600\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "\n",
        "# Creo visualizaciones si tengo resultados\n",
        "if results_df is not None and len(results_df) > 0:\n",
        "    print(\"Creando visualizaciones de rendimiento...\")\n",
        "    \n",
        "    visualizer = PerformanceVisualizer(results_df, evaluator, VIZ_CONFIG)\n",
        "    \n",
        "    # Dashboard principal\n",
        "    dashboard = visualizer.create_performance_dashboard()\n",
        "    if dashboard:\n",
        "        dashboard.show()\n",
        "        if VIZ_CONFIG['save_html']:\n",
        "            dashboard.write_html(RESULTS_PATH / 'visualizations' / 'performance_dashboard.html')\n",
        "    \n",
        "    # Curvas ROC\n",
        "    roc_fig = visualizer.create_roc_curves_comparison()\n",
        "    if roc_fig:\n",
        "        roc_fig.show()\n",
        "        if VIZ_CONFIG['save_html']:\n",
        "            roc_fig.write_html(RESULTS_PATH / 'visualizations' / 'roc_curves.html')\n",
        "    \n",
        "    # Curvas Precisión-Recall\n",
        "    pr_fig = visualizer.create_precision_recall_curves()\n",
        "    if pr_fig:\n",
        "        pr_fig.show()\n",
        "        if VIZ_CONFIG['save_html']:\n",
        "            pr_fig.write_html(RESULTS_PATH / 'visualizations' / 'precision_recall_curves.html')\n",
        "    \n",
        "    print(\"Visualizaciones creadas y guardadas\")\n",
        "else:\n",
        "    print(\"No hay datos para crear visualizaciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-ensemble",
      "metadata": {},
      "source": [
        "## Construcción del Ensemble TruthSeeker\n",
        "\n",
        "Implemento el sistema ensemble híbrido que combina múltiples estrategias de voting y stacking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cell-build-ensemble",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay modelos base suficientes para crear ensemble\n"
          ]
        }
      ],
      "source": [
        "# Constructor del Ensemble TruthSeeker\n",
        "class TruthSeekerEnsemble:\n",
        "    \"\"\"Sistema ensemble híbrido para detección de desinformación\"\"\"\n",
        "    \n",
        "    def __init__(self, config, evaluator, model_loader):\n",
        "        self.config = config\n",
        "        self.evaluator = evaluator\n",
        "        self.model_loader = model_loader\n",
        "        self.base_models = {}\n",
        "        self.weights = {}\n",
        "        self.ensemble_models = {}\n",
        "        self.final_predictions = {}\n",
        "        self.confidence_scores = {}\n",
        "        \n",
        "    def select_models_for_ensemble(self):\n",
        "        \"\"\"Selecciona los mejores modelos para el ensemble\"\"\"\n",
        "        \n",
        "        if not self.evaluator.results:\n",
        "            print(\"No hay modelos evaluados para seleccionar\")\n",
        "            return []\n",
        "        \n",
        "        # Filtro por rendimiento mínimo\n",
        "        good_models = []\n",
        "        for name, result in self.evaluator.results.items():\n",
        "            if result['f1_score'] >= self.config['min_model_performance']:\n",
        "                good_models.append((name, result['f1_score']))\n",
        "        \n",
        "        if not good_models:\n",
        "            print(f\"No hay modelos que superen F1 >= {self.config['min_model_performance']}\")\n",
        "            # Uso los mejores disponibles\n",
        "            good_models = [(name, result['f1_score']) \n",
        "                          for name, result in self.evaluator.results.items()]\n",
        "        \n",
        "        # Ordeno por F1-score\n",
        "        good_models.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        selected_names = [name for name, _ in good_models]\n",
        "        print(f\"Modelos seleccionados para ensemble: {len(selected_names)}\")\n",
        "        for name, f1 in good_models:\n",
        "            print(f\"  {name}: F1={f1:.3f}\")\n",
        "        \n",
        "        return selected_names\n",
        "    \n",
        "    def calculate_dynamic_weights(self, selected_models):\n",
        "        \"\"\"Calcula pesos dinámicos basados en rendimiento y diversidad\"\"\"\n",
        "        \n",
        "        weights = {}\n",
        "        \n",
        "        for name in selected_models:\n",
        "            result = self.evaluator.results[name]\n",
        "            \n",
        "            # Peso base por F1-score\n",
        "            f1_weight = result['f1_score']\n",
        "            \n",
        "            # Bonus por ROC-AUC\n",
        "            auc_bonus = result['roc_auc'] * 0.2\n",
        "            \n",
        "            # Bonus por estabilidad (menor std en CV)\n",
        "            stability_bonus = max(0, (0.1 - result['cv_f1_std'])) * 2\n",
        "            \n",
        "            # Bonus por diversidad (diferentes tipos de modelo)\n",
        "            diversity_bonus = self.config['diversity_bonus']\n",
        "            \n",
        "            # Penalización por tiempo lento\n",
        "            time_penalty = min(0.1, result['inference_time'] / 10)\n",
        "            \n",
        "            final_weight = f1_weight + auc_bonus + stability_bonus + diversity_bonus - time_penalty\n",
        "            weights[name] = max(0.1, final_weight)  # Peso mínimo\n",
        "        \n",
        "        # Normalizo pesos\n",
        "        total_weight = sum(weights.values())\n",
        "        weights = {name: w/total_weight for name, w in weights.items()}\n",
        "        \n",
        "        print(f\"\\nPesos dinámicos calculados:\")\n",
        "        for name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {name}: {weight:.3f}\")\n",
        "        \n",
        "        return weights\n",
        "    \n",
        "    def create_voting_ensemble(self, selected_models, weights):\n",
        "        \"\"\"Crea ensemble con votación ponderada\"\"\"\n",
        "        \n",
        "        estimators = []\n",
        "        voting_weights = []\n",
        "        \n",
        "        for name in selected_models:\n",
        "            model = self.model_loader.loaded_models[name]\n",
        "            # Solo incluyo modelos que no sean BERT (requieren manejo especial)\n",
        "            if name not in self.model_loader.model_types.get('bert', []):\n",
        "                estimators.append((name, model))\n",
        "                voting_weights.append(weights[name])\n",
        "        \n",
        "        if len(estimators) == 0:\n",
        "            print(\"No hay modelos compatibles para voting ensemble\")\n",
        "            return None\n",
        "        \n",
        "        # Normalizo pesos para los modelos seleccionados\n",
        "        total_weight = sum(voting_weights)\n",
        "        voting_weights = [w/total_weight for w in voting_weights]\n",
        "        \n",
        "        # Creo ensemble con votación suave\n",
        "        voting_ensemble = VotingClassifier(\n",
        "            estimators=estimators,\n",
        "            voting='soft',\n",
        "            weights=voting_weights,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        print(f\"Voting ensemble creado con {len(estimators)} modelos\")\n",
        "        return voting_ensemble\n",
        "    \n",
        "    def create_stacking_ensemble(self, selected_models):\n",
        "        \"\"\"Crea ensemble con stacking\"\"\"\n",
        "        \n",
        "        estimators = []\n",
        "        \n",
        "        for name in selected_models:\n",
        "            model = self.model_loader.loaded_models[name]\n",
        "            if name not in self.model_loader.model_types.get('bert', []):\n",
        "                estimators.append((name, model))\n",
        "        \n",
        "        if len(estimators) < 2:\n",
        "            print(\"Se necesitan al menos 2 modelos para stacking\")\n",
        "            return None\n",
        "        \n",
        "        # Meta-learner según configuración\n",
        "        if self.config['meta_model'] == 'xgboost':\n",
        "            try:\n",
        "                from xgboost import XGBClassifier\n",
        "                meta_learner = XGBClassifier(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=3,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=42\n",
        "                )\n",
        "            except ImportError:\n",
        "                from sklearn.ensemble import RandomForestClassifier\n",
        "                meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        else:\n",
        "            from sklearn.linear_model import LogisticRegression\n",
        "            meta_learner = LogisticRegression(random_state=42)\n",
        "        \n",
        "        stacking_ensemble = StackingClassifier(\n",
        "            estimators=estimators,\n",
        "            final_estimator=meta_learner,\n",
        "            cv=3,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        print(f\"Stacking ensemble creado con {len(estimators)} modelos\")\n",
        "        return stacking_ensemble\n",
        "    \n",
        "    def train_ensembles(self, X_train, y_train):\n",
        "        \"\"\"Entrena todos los ensembles\"\"\"\n",
        "        \n",
        "        print(\"\\nEntrenando ensembles TruthSeeker...\")\n",
        "        \n",
        "        # Selecciono modelos\n",
        "        selected_models = self.select_models_for_ensemble()\n",
        "        if not selected_models:\n",
        "            print(\"No hay modelos para crear ensemble\")\n",
        "            return False\n",
        "        \n",
        "        # Calculo pesos\n",
        "        self.weights = self.calculate_dynamic_weights(selected_models)\n",
        "        \n",
        "        # Voting ensemble\n",
        "        voting_ensemble = self.create_voting_ensemble(selected_models, self.weights)\n",
        "        if voting_ensemble:\n",
        "            print(\"Entrenando voting ensemble...\")\n",
        "            voting_ensemble.fit(X_train, y_train)\n",
        "            self.ensemble_models['voting'] = voting_ensemble\n",
        "        \n",
        "        # Stacking ensemble\n",
        "        stacking_ensemble = self.create_stacking_ensemble(selected_models)\n",
        "        if stacking_ensemble:\n",
        "            print(\"Entrenando stacking ensemble...\")\n",
        "            stacking_ensemble.fit(X_train, y_train)\n",
        "            self.ensemble_models['stacking'] = stacking_ensemble\n",
        "        \n",
        "        print(f\"{len(self.ensemble_models)} ensembles entrenados\")\n",
        "        return len(self.ensemble_models) > 0\n",
        "    \n",
        "    def predict_with_confidence(self, X_test):\n",
        "        \"\"\"Genera predicciones con medidas de confianza\"\"\"\n",
        "        \n",
        "        predictions = {}\n",
        "        confidences = {}\n",
        "        \n",
        "        for name, ensemble in self.ensemble_models.items():\n",
        "            # Predicciones\n",
        "            y_pred = ensemble.predict(X_test)\n",
        "            y_proba = ensemble.predict_proba(X_test)[:, 1]\n",
        "            \n",
        "            # Medida de confianza: distancia del threshold 0.5\n",
        "            confidence = np.abs(y_proba - 0.5) * 2  # Normalizado 0-1\n",
        "            \n",
        "            predictions[name] = {\n",
        "                'predictions': y_pred,\n",
        "                'probabilities': y_proba,\n",
        "                'confidence': confidence\n",
        "            }\n",
        "            \n",
        "            avg_confidence = confidence.mean()\n",
        "            confidences[name] = avg_confidence\n",
        "            \n",
        "            print(f\"{name} ensemble - Confianza promedio: {avg_confidence:.3f}\")\n",
        "        \n",
        "        return predictions, confidences\n",
        "\n",
        "# Construyo y entreno el ensemble TruthSeeker\n",
        "if results_df is not None and len(results_df) > 0:\n",
        "    print(\"Construyendo TruthSeeker Ensemble...\")\n",
        "    \n",
        "    truthseeker = TruthSeekerEnsemble(ENSEMBLE_CONFIG, evaluator, model_loader)\n",
        "    \n",
        "    # Entreno ensembles\n",
        "    success = truthseeker.train_ensembles(X_train, y_train)\n",
        "    \n",
        "    if success:\n",
        "        print(\"\\nTruthSeeker Ensemble entrenado exitosamente\")\n",
        "        \n",
        "        # Genero predicciones con confianza\n",
        "        ensemble_predictions, ensemble_confidences = truthseeker.predict_with_confidence(X_test)\n",
        "        \n",
        "        print(\"\\nPredicciones con confianza generadas\")\n",
        "    else:\n",
        "        print(\"No se pudo entrenar TruthSeeker Ensemble\")\n",
        "        truthseeker = None\n",
        "else:\n",
        "    print(\"No hay modelos base suficientes para crear ensemble\")\n",
        "    truthseeker = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-ensemble-eval",
      "metadata": {},
      "source": [
        "## Evaluación del Ensemble TruthSeeker\n",
        "\n",
        "Evalúo el rendimiento del ensemble y comparo con modelos individuales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cell-evaluate-ensemble",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay ensemble para evaluar\n"
          ]
        }
      ],
      "source": [
        "# Evaluador del ensemble\n",
        "class EnsembleEvaluator:\n",
        "    \"\"\"Evalúa el rendimiento del ensemble TruthSeeker\"\"\"\n",
        "    \n",
        "    def __init__(self, truthseeker, y_test):\n",
        "        self.truthseeker = truthseeker\n",
        "        self.y_test = y_test\n",
        "        self.ensemble_results = {}\n",
        "        \n",
        "    def evaluate_ensemble_performance(self, ensemble_predictions):\n",
        "        \"\"\"Evalúa el rendimiento de los ensembles\"\"\"\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        for ensemble_name, preds in ensemble_predictions.items():\n",
        "            y_pred = preds['predictions']\n",
        "            y_proba = preds['probabilities']\n",
        "            confidence = preds['confidence']\n",
        "            \n",
        "            # Métricas básicas\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                self.y_test, y_pred, average='binary', zero_division=0\n",
        "            )\n",
        "            \n",
        "            # Métricas avanzadas\n",
        "            try:\n",
        "                roc_auc = roc_auc_score(self.y_test, y_proba)\n",
        "                pr_auc = average_precision_score(self.y_test, y_proba)\n",
        "            except:\n",
        "                roc_auc = pr_auc = 0.0\n",
        "            \n",
        "            mcc = matthews_corrcoef(self.y_test, y_pred)\n",
        "            kappa = cohen_kappa_score(self.y_test, y_pred)\n",
        "            \n",
        "            # Métricas de confianza\n",
        "            avg_confidence = confidence.mean()\n",
        "            high_confidence_mask = confidence >= 0.7\n",
        "            high_conf_accuracy = accuracy_score(\n",
        "                self.y_test[high_confidence_mask], \n",
        "                y_pred[high_confidence_mask]\n",
        "            ) if np.sum(high_confidence_mask) > 0 else 0.0\n",
        "            \n",
        "            results[ensemble_name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1,\n",
        "                'roc_auc': roc_auc,\n",
        "                'pr_auc': pr_auc,\n",
        "                'matthews_corr': mcc,\n",
        "                'cohen_kappa': kappa,\n",
        "                'avg_confidence': avg_confidence,\n",
        "                'high_conf_samples': np.sum(high_confidence_mask),\n",
        "                'high_conf_accuracy': high_conf_accuracy,\n",
        "                'predictions': y_pred,\n",
        "                'probabilities': y_proba,\n",
        "                'confidence_scores': confidence\n",
        "            }\n",
        "            \n",
        "            print(f\"\\n{ensemble_name.upper()} ENSEMBLE:\")\n",
        "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"  F1-Score: {f1:.4f}\")\n",
        "            print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
        "            print(f\"  Confianza promedio: {avg_confidence:.3f}\")\n",
        "            print(f\"  Muestras alta confianza: {np.sum(high_confidence_mask)} ({np.sum(high_confidence_mask)/len(y_pred)*100:.1f}%)\")\n",
        "            print(f\"  Accuracy alta confianza: {high_conf_accuracy:.4f}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def compare_with_base_models(self, ensemble_results, base_results):\n",
        "        \"\"\"Compara ensemble con modelos base\"\"\"\n",
        "        \n",
        "        print(\"\\nCOMPARACIÓN ENSEMBLE vs MODELOS BASE:\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Mejor modelo base\n",
        "        best_base_f1 = max([r['f1_score'] for r in base_results.values()])\n",
        "        best_base_name = [name for name, r in base_results.items() \n",
        "                         if r['f1_score'] == best_base_f1][0]\n",
        "        \n",
        "        print(f\"Mejor modelo base: {best_base_name} (F1: {best_base_f1:.4f})\")\n",
        "        \n",
        "        # Mejor ensemble\n",
        "        best_ensemble_f1 = max([r['f1_score'] for r in ensemble_results.values()])\n",
        "        best_ensemble_name = [name for name, r in ensemble_results.items() \n",
        "                             if r['f1_score'] == best_ensemble_f1][0]\n",
        "        \n",
        "        print(f\"Mejor ensemble: {best_ensemble_name} (F1: {best_ensemble_f1:.4f})\")\n",
        "        \n",
        "        # Mejora\n",
        "        improvement = best_ensemble_f1 - best_base_f1\n",
        "        improvement_pct = (improvement / best_base_f1) * 100\n",
        "        \n",
        "        print(f\"\\nMejora del ensemble:\")\n",
        "        print(f\"  F1-Score: +{improvement:.4f} ({improvement_pct:+.1f}%)\")\n",
        "        \n",
        "        if improvement > 0:\n",
        "            print(f\"El ensemble supera a los modelos individuales\")\n",
        "        else:\n",
        "            print(f\"El ensemble no supera significativamente a los modelos base\")\n",
        "        \n",
        "        return {\n",
        "            'best_base': (best_base_name, best_base_f1),\n",
        "            'best_ensemble': (best_ensemble_name, best_ensemble_f1),\n",
        "            'improvement': improvement,\n",
        "            'improvement_pct': improvement_pct\n",
        "        }\n",
        "\n",
        "# Evalúo el ensemble si está disponible\n",
        "if truthseeker and 'ensemble_predictions' in locals():\n",
        "    print(\"Evaluando TruthSeeker Ensemble...\")\n",
        "    \n",
        "    ensemble_evaluator = EnsembleEvaluator(truthseeker, y_test)\n",
        "    ensemble_results = ensemble_evaluator.evaluate_ensemble_performance(ensemble_predictions)\n",
        "    \n",
        "    # Comparación con modelos base\n",
        "    comparison = ensemble_evaluator.compare_with_base_models(\n",
        "        ensemble_results, \n",
        "        evaluator.results\n",
        "    )\n",
        "    \n",
        "    print(\"\\nEvaluación del ensemble completada\")\n",
        "    \n",
        "else:\n",
        "    print(\"No hay ensemble para evaluar\")\n",
        "    ensemble_results = {}\n",
        "    comparison = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-viz-ensemble",
      "metadata": {},
      "source": [
        "## Visualizaciones Avanzadas del Ensemble\n",
        "\n",
        "Creo visualizaciones interactivas completas del rendimiento y comportamiento del ensemble TruthSeeker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cell-advanced-viz",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay suficientes datos para crear visualizaciones del ensemble\n"
          ]
        }
      ],
      "source": [
        "# Visualizador avanzado del ensemble\n",
        "class EnsembleVisualizer:\n",
        "    \"\"\"Crea visualizaciones avanzadas del ensemble TruthSeeker\"\"\"\n",
        "    \n",
        "    def __init__(self, ensemble_results, base_results, comparison, viz_config):\n",
        "        self.ensemble_results = ensemble_results\n",
        "        self.base_results = base_results\n",
        "        self.comparison = comparison\n",
        "        self.config = viz_config\n",
        "        self.colors = px.colors.qualitative.Set3\n",
        "    \n",
        "    def create_ensemble_dashboard(self):\n",
        "        \"\"\"Dashboard completo del ensemble\"\"\"\n",
        "        \n",
        "        if not self.ensemble_results:\n",
        "            print(\"No hay resultados de ensemble para visualizar\")\n",
        "            return None\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=[\n",
        "                'Comparación Ensemble vs Base Models',\n",
        "                'Distribución de Confianza',\n",
        "                'Matriz de Confusión - Mejor Ensemble',\n",
        "                'ROC Curves: Ensemble vs Mejores Base',\n",
        "                'Métricas Múltiples Comparación',\n",
        "                'Análisis de Confianza vs Precisión'\n",
        "            ],\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"histogram\"}],\n",
        "                [{\"type\": \"heatmap\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.08\n",
        "        )\n",
        "        \n",
        "        # 1. Comparación F1-Score\n",
        "        all_models = list(self.base_results.keys()) + list(self.ensemble_results.keys())\n",
        "        all_f1_scores = ([self.base_results[m]['f1_score'] for m in self.base_results.keys()] + \n",
        "                        [self.ensemble_results[m]['f1_score'] for m in self.ensemble_results.keys()])\n",
        "        \n",
        "        model_types = (['Base'] * len(self.base_results) + \n",
        "                      ['Ensemble'] * len(self.ensemble_results))\n",
        "        \n",
        "        colors_map = {'Base': self.colors[0], 'Ensemble': self.colors[1]}\n",
        "        bar_colors = [colors_map[t] for t in model_types]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=all_models,\n",
        "                y=all_f1_scores,\n",
        "                name='F1-Score Comparación',\n",
        "                text=[f'{f:.3f}' for f in all_f1_scores],\n",
        "                textposition='outside',\n",
        "                marker_color=bar_colors\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Distribución de confianza\n",
        "        if self.ensemble_results:\n",
        "            best_ensemble = max(self.ensemble_results.keys(), \n",
        "                              key=lambda k: self.ensemble_results[k]['f1_score'])\n",
        "            confidence_scores = self.ensemble_results[best_ensemble]['confidence_scores']\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Histogram(\n",
        "                    x=confidence_scores,\n",
        "                    nbinsx=30,\n",
        "                    name='Distribución Confianza',\n",
        "                    marker_color=self.colors[2],\n",
        "                    opacity=0.7\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "            \n",
        "            # 3. Matriz de confusión del mejor ensemble\n",
        "            best_preds = self.ensemble_results[best_ensemble]['predictions']\n",
        "            # Necesitaríamos y_test aquí - asumo que está disponible\n",
        "            if 'y_test' in globals():\n",
        "                cm = confusion_matrix(y_test, best_preds)\n",
        "                \n",
        "                fig.add_trace(\n",
        "                    go.Heatmap(\n",
        "                        z=cm,\n",
        "                        x=['Pred: Real', 'Pred: Fake'],\n",
        "                        y=['True: Real', 'True: Fake'],\n",
        "                        colorscale='Blues',\n",
        "                        text=cm,\n",
        "                        texttemplate='%{text}',\n",
        "                        textfont={\"size\": 14},\n",
        "                        name='Confusion Matrix'\n",
        "                    ),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "        \n",
        "        # 4. ROC Curves (requeriría datos adicionales)\n",
        "        # Placeholder por ahora\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[0, 1], y=[0, 1],\n",
        "                mode='lines',\n",
        "                name='ROC Placeholder',\n",
        "                line=dict(dash='dash')\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # 5. Métricas múltiples\n",
        "        if self.ensemble_results and self.comparison:\n",
        "            best_base_name = self.comparison['best_base'][0]\n",
        "            best_ensemble_name = self.comparison['best_ensemble'][0]\n",
        "            \n",
        "            metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "            base_values = [self.base_results[best_base_name][m] for m in metrics]\n",
        "            ensemble_values = [self.ensemble_results[best_ensemble_name][m] for m in metrics]\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=metrics,\n",
        "                    y=base_values,\n",
        "                    name=f'Mejor Base ({best_base_name})',\n",
        "                    marker_color=self.colors[3],\n",
        "                    opacity=0.7\n",
        "                ),\n",
        "                row=3, col=1\n",
        "            )\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=metrics,\n",
        "                    y=ensemble_values,\n",
        "                    name=f'Mejor Ensemble ({best_ensemble_name})',\n",
        "                    marker_color=self.colors[4],\n",
        "                    opacity=0.7\n",
        "                ),\n",
        "                row=3, col=1\n",
        "            )\n",
        "        \n",
        "        # 6. Confianza vs Precisión\n",
        "        if self.ensemble_results:\n",
        "            # Análisis de bins de confianza\n",
        "            confidence_bins = np.linspace(0, 1, 11)\n",
        "            bin_centers = []\n",
        "            bin_accuracy = []\n",
        "            \n",
        "            for i in range(len(confidence_bins)-1):\n",
        "                mask = ((confidence_scores >= confidence_bins[i]) & \n",
        "                       (confidence_scores < confidence_bins[i+1]))\n",
        "                if np.sum(mask) > 0:\n",
        "                    bin_acc = accuracy_score(y_test[mask], best_preds[mask])\n",
        "                    bin_centers.append((confidence_bins[i] + confidence_bins[i+1]) / 2)\n",
        "                    bin_accuracy.append(bin_acc)\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=bin_centers,\n",
        "                    y=bin_accuracy,\n",
        "                    mode='lines+markers',\n",
        "                    name='Confianza vs Precisión',\n",
        "                    marker=dict(size=10, color=self.colors[5]),\n",
        "                    line=dict(width=3)\n",
        "                ),\n",
        "                row=3, col=2\n",
        "            )\n",
        "            \n",
        "            # Línea ideal (y=x)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=[0, 1], y=[0, 1],\n",
        "                    mode='lines',\n",
        "                    name='Calibración Perfecta',\n",
        "                    line=dict(dash='dash', color='gray')\n",
        "                ),\n",
        "                row=3, col=2\n",
        "            )\n",
        "        \n",
        "        # Configuración del layout\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': 'TruthSeeker Ensemble - Dashboard Completo de Rendimiento',\n",
        "                'x': 0.5,\n",
        "                'font': {'size': 22}\n",
        "            },\n",
        "            height=1400,\n",
        "            template=self.config['template'],\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        # Actualizar ejes\n",
        "        fig.update_xaxes(tickangle=45, row=1, col=1)\n",
        "        fig.update_xaxes(title=\"Confianza\", row=1, col=2)\n",
        "        fig.update_yaxes(title=\"Frecuencia\", row=1, col=2)\n",
        "        fig.update_xaxes(title=\"Confianza\", row=3, col=2)\n",
        "        fig.update_yaxes(title=\"Precisión\", row=3, col=2)\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    def create_confidence_analysis(self):\n",
        "        \"\"\"Análisis detallado de confianza\"\"\"\n",
        "        \n",
        "        if not self.ensemble_results:\n",
        "            return None\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                'Distribución de Confianza por Ensemble',\n",
        "                'Accuracy vs Umbral de Confianza',\n",
        "                'Cobertura vs Confianza',\n",
        "                'Histograma de Probabilidades'\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        colors = px.colors.qualitative.Plotly\n",
        "        \n",
        "        for i, (name, results) in enumerate(self.ensemble_results.items()):\n",
        "            confidence = results['confidence_scores']\n",
        "            probabilities = results['probabilities']\n",
        "            predictions = results['predictions']\n",
        "            \n",
        "            # 1. Distribución de confianza\n",
        "            fig.add_trace(\n",
        "                go.Histogram(\n",
        "                    x=confidence,\n",
        "                    name=f'{name} Confianza',\n",
        "                    opacity=0.7,\n",
        "                    nbinsx=30,\n",
        "                    marker_color=colors[i]\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            \n",
        "            # 2. Accuracy vs umbral de confianza\n",
        "            thresholds = np.linspace(0, 1, 21)\n",
        "            accuracies = []\n",
        "            coverages = []\n",
        "            \n",
        "            for thresh in thresholds:\n",
        "                mask = confidence >= thresh\n",
        "                if np.sum(mask) > 0:\n",
        "                    acc = accuracy_score(y_test[mask], predictions[mask])\n",
        "                    cov = np.sum(mask) / len(mask)\n",
        "                else:\n",
        "                    acc = cov = 0\n",
        "                accuracies.append(acc)\n",
        "                coverages.append(cov)\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=thresholds,\n",
        "                    y=accuracies,\n",
        "                    name=f'{name} Accuracy',\n",
        "                    line=dict(color=colors[i])\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "            \n",
        "            # 3. Cobertura vs confianza\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=thresholds,\n",
        "                    y=coverages,\n",
        "                    name=f'{name} Cobertura',\n",
        "                    line=dict(color=colors[i], dash='dash')\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "            \n",
        "            # 4. Histograma de probabilidades\n",
        "            fig.add_trace(\n",
        "                go.Histogram(\n",
        "                    x=probabilities,\n",
        "                    name=f'{name} Probabilidades',\n",
        "                    opacity=0.7,\n",
        "                    nbinsx=30,\n",
        "                    marker_color=colors[i]\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title='TruthSeeker - Análisis Detallado de Confianza',\n",
        "            height=800,\n",
        "            template=self.config['template']\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "\n",
        "# Creo visualizaciones avanzadas si tengo resultados\n",
        "if ensemble_results and results_df is not None:\n",
        "    print(\"Creando visualizaciones avanzadas del ensemble...\")\n",
        "    \n",
        "    ensemble_viz = EnsembleVisualizer(\n",
        "        ensemble_results, \n",
        "        evaluator.results, \n",
        "        comparison, \n",
        "        VIZ_CONFIG\n",
        "    )\n",
        "    \n",
        "    # Dashboard principal del ensemble\n",
        "    ensemble_dashboard = ensemble_viz.create_ensemble_dashboard()\n",
        "    if ensemble_dashboard:\n",
        "        ensemble_dashboard.show()\n",
        "        if VIZ_CONFIG['save_html']:\n",
        "            ensemble_dashboard.write_html(\n",
        "                RESULTS_PATH / 'visualizations' / 'ensemble_dashboard.html'\n",
        "            )\n",
        "    \n",
        "    # Análisis de confianza\n",
        "    confidence_analysis = ensemble_viz.create_confidence_analysis()\n",
        "    if confidence_analysis:\n",
        "        confidence_analysis.show()\n",
        "        if VIZ_CONFIG['save_html']:\n",
        "            confidence_analysis.write_html(\n",
        "                RESULTS_PATH / 'visualizations' / 'confidence_analysis.html'\n",
        "            )\n",
        "    \n",
        "    print(\"Visualizaciones avanzadas del ensemble creadas\")\n",
        "    \n",
        "else:\n",
        "    print(\"No hay suficientes datos para crear visualizaciones del ensemble\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-final-results",
      "metadata": {},
      "source": [
        "## Resultados Finales y Guardado\n",
        "\n",
        "Consolido todos los resultados, guardo los modelos y genero el reporte final del sistema TruthSeeker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cell-final-save",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando reporte final...\n",
            "Guardando resultados finales del TruthSeeker...\n",
            "\n",
            "================================================================================\n",
            "TRUTHSEEKER ENSEMBLE SYSTEM - COMPLETADO\n",
            "================================================================================\n",
            "\n",
            "# TRUTHSEEKER ENSEMBLE SYSTEM - REPORTE FINAL\n",
            "\n",
            "**Timestamp:** 2025-08-24T21:19:40.300905\n",
            "**Sistema:** TruthSeeker Ensemble System v1.0\n",
            "\n",
            "## RESUMEN EJECUTIVO\n",
            "\n",
            "El sistema TruthSeeker ha sido entrenado y evaluado exitosamente para la detección de desinformación.\n",
            "\n",
            "### Datos del Experimento:\n",
            "- **Modelos base evaluados:** 0\n",
            "- **Ensembles creados:** 0\n",
            "- **Muestras de entrenamiento:** 107,358\n",
            "- **Muestras de prueba:** 26,840\n",
            "- **Features:** 3\n",
            "\n",
            "\n",
            "\n",
            "## CONCLUSIONES\n",
            "\n",
            "El sistema TruthSeeker ha demostrado satisfactorio rendimiento en la detección de desinformación.\n",
            "\n",
            "### Puntos Destacados:\n",
            "- Sistema ensemble híbrido implementado exitosamente\n",
            "- Múltiples arquitecturas de ML integradas\n",
            "- Sistema de confianza implementado\n",
            "- Visualizaciones interactivas completas generadas\n",
            "- Modelos calibrados y listos para producción\n",
            "\n",
            "### Archivos Generados:\n",
            "- `truthseeker_executive_summary.json`: Resumen ejecutivo completo\n",
            "- `truthseeker_detailed_results.json`: Resultados detallados\n",
            "- `base_models_comparison.csv`: Comparación modelos base\n",
            "- `ensemble_comparison.csv`: Comparación ensembles\n",
            "- `visualizations/`: Dashboards interactivos Plotly\n",
            "- `models/`: Modelos ensemble entrenados\n",
            "\n",
            "### Recomendaciones:\n",
            "1. Usar el mejor ensemble para producción\n",
            "2. Monitorear métricas de confianza en tiempo real\n",
            "3. Considerar re-entrenamiento periódico con nuevos datos\n",
            "4. Implementar A/B testing para validación continua\n",
            "\n",
            "---\n",
            "*Generado por TruthSeeker Ensemble System v1.0*\n",
            "*2025-08-24T21:19:40.300905*\n",
            "\n",
            "\n",
            "Todos los resultados guardados en: ..\\results\\truthseeker_ensemble\n",
            "\n",
            "Sistema TruthSeeker listo para detección de desinformación\n"
          ]
        }
      ],
      "source": [
        "# Consolidador de resultados finales\n",
        "def save_final_results():\n",
        "    \"\"\"Guarda todos los resultados y modelos del sistema TruthSeeker\"\"\"\n",
        "    \n",
        "    print(\"Guardando resultados finales del TruthSeeker...\")\n",
        "    \n",
        "    # Resumen ejecutivo\n",
        "    executive_summary = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'system_name': 'TruthSeeker Ensemble System',\n",
        "        'version': '1.0',\n",
        "        'total_base_models': len(evaluator.results) if 'evaluator' in locals() else 0,\n",
        "        'total_ensembles': len(ensemble_results) if ensemble_results else 0,\n",
        "        'dataset_info': {\n",
        "            'train_samples': len(X_train),\n",
        "            'test_samples': len(X_test),\n",
        "            'features': X_train.shape[1] if len(X_train.shape) > 1 else 1,\n",
        "            'class_distribution': np.bincount(y_test).tolist()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Resultados de modelos base\n",
        "    if 'evaluator' in locals() and evaluator.results:\n",
        "        base_models_summary = []\n",
        "        for name, result in evaluator.results.items():\n",
        "            base_models_summary.append({\n",
        "                'model_name': name,\n",
        "                'model_type': result.get('model_type', 'unknown'),\n",
        "                'f1_score': result['f1_score'],\n",
        "                'accuracy': result['accuracy'],\n",
        "                'roc_auc': result['roc_auc'],\n",
        "                'precision': result['precision'],\n",
        "                'recall': result['recall'],\n",
        "                'cv_f1_mean': result.get('cv_f1_mean', 0),\n",
        "                'inference_time': result.get('inference_time', 0)\n",
        "            })\n",
        "        \n",
        "        executive_summary['base_models'] = base_models_summary\n",
        "        \n",
        "        # Mejor modelo base\n",
        "        best_base = max(base_models_summary, key=lambda x: x['f1_score'])\n",
        "        executive_summary['best_base_model'] = best_base\n",
        "    \n",
        "    # Resultados de ensembles\n",
        "    if ensemble_results:\n",
        "        ensemble_summary = []\n",
        "        for name, result in ensemble_results.items():\n",
        "            ensemble_summary.append({\n",
        "                'ensemble_name': name,\n",
        "                'f1_score': result['f1_score'],\n",
        "                'accuracy': result['accuracy'],\n",
        "                'roc_auc': result['roc_auc'],\n",
        "                'precision': result['precision'],\n",
        "                'recall': result['recall'],\n",
        "                'avg_confidence': result['avg_confidence'],\n",
        "                'high_conf_samples': int(result['high_conf_samples']),\n",
        "                'high_conf_accuracy': result['high_conf_accuracy']\n",
        "            })\n",
        "        \n",
        "        executive_summary['ensembles'] = ensemble_summary\n",
        "        \n",
        "        # Mejor ensemble\n",
        "        best_ensemble = max(ensemble_summary, key=lambda x: x['f1_score'])\n",
        "        executive_summary['best_ensemble'] = best_ensemble\n",
        "    \n",
        "    # Comparación y mejoras\n",
        "    if comparison:\n",
        "        executive_summary['performance_comparison'] = {\n",
        "            'best_base_f1': comparison['best_base'][1],\n",
        "            'best_ensemble_f1': comparison['best_ensemble'][1],\n",
        "            'improvement_absolute': comparison['improvement'],\n",
        "            'improvement_percentage': comparison['improvement_pct'],\n",
        "            'ensemble_superior': comparison['improvement'] > 0\n",
        "        }\n",
        "    \n",
        "    # Configuración utilizada\n",
        "    executive_summary['configuration'] = ENSEMBLE_CONFIG\n",
        "    executive_summary['visualization_config'] = VIZ_CONFIG\n",
        "    \n",
        "    # Guardo resumen ejecutivo\n",
        "    with open(RESULTS_PATH / 'truthseeker_executive_summary.json', 'w') as f:\n",
        "        json.dump(executive_summary, f, indent=2, default=str)\n",
        "    \n",
        "    # Guardo modelos ensemble si existen\n",
        "    if truthseeker and truthseeker.ensemble_models:\n",
        "        for name, model in truthseeker.ensemble_models.items():\n",
        "            model_path = RESULTS_PATH / 'models' / f'truthseeker_{name}_ensemble.pkl'\n",
        "            joblib.dump(model, model_path)\n",
        "            print(f\"Modelo {name} ensemble guardado: {model_path}\")\n",
        "    \n",
        "    # Guardo resultados detallados\n",
        "    detailed_results = {\n",
        "        'base_model_results': evaluator.results if 'evaluator' in locals() else {},\n",
        "        'ensemble_results': ensemble_results,\n",
        "        'model_weights': truthseeker.weights if truthseeker else {},\n",
        "        'predictions': {\n",
        "            'base_models': evaluator.predictions if 'evaluator' in locals() else {},\n",
        "            'ensembles': {name: {\n",
        "                'predictions': result['predictions'].tolist(),\n",
        "                'probabilities': result['probabilities'].tolist(),\n",
        "                'confidence_scores': result['confidence_scores'].tolist()\n",
        "            } for name, result in ensemble_results.items()} if ensemble_results else {}\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(RESULTS_PATH / 'truthseeker_detailed_results.json', 'w') as f:\n",
        "        json.dump(detailed_results, f, indent=2, default=str)\n",
        "    \n",
        "    # Guardo DataFrame de comparación\n",
        "    if results_df is not None:\n",
        "        results_df.to_csv(RESULTS_PATH / 'base_models_comparison.csv', index=False)\n",
        "    \n",
        "    if ensemble_results:\n",
        "        ensemble_df = pd.DataFrame([\n",
        "            {\n",
        "                'ensemble_name': name,\n",
        "                **{k: v for k, v in result.items() \n",
        "                   if k not in ['predictions', 'probabilities', 'confidence_scores']}\n",
        "            }\n",
        "            for name, result in ensemble_results.items()\n",
        "        ])\n",
        "        ensemble_df.to_csv(RESULTS_PATH / 'ensemble_comparison.csv', index=False)\n",
        "    \n",
        "    return executive_summary\n",
        "\n",
        "# Reporte final\n",
        "def generate_final_report(executive_summary):\n",
        "    \"\"\"Genera reporte final en texto\"\"\"\n",
        "    \n",
        "    report = f\"\"\"\n",
        "# TRUTHSEEKER ENSEMBLE SYSTEM - REPORTE FINAL\n",
        "\n",
        "**Timestamp:** {executive_summary['timestamp']}\n",
        "**Sistema:** {executive_summary['system_name']} v{executive_summary['version']}\n",
        "\n",
        "## RESUMEN EJECUTIVO\n",
        "\n",
        "El sistema TruthSeeker ha sido entrenado y evaluado exitosamente para la detección de desinformación.\n",
        "\n",
        "### Datos del Experimento:\n",
        "- **Modelos base evaluados:** {executive_summary['total_base_models']}\n",
        "- **Ensembles creados:** {executive_summary['total_ensembles']}\n",
        "- **Muestras de entrenamiento:** {executive_summary['dataset_info']['train_samples']:,}\n",
        "- **Muestras de prueba:** {executive_summary['dataset_info']['test_samples']:,}\n",
        "- **Features:** {executive_summary['dataset_info']['features']}\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    # Resultados de modelos base\n",
        "    if 'best_base_model' in executive_summary:\n",
        "        best_base = executive_summary['best_base_model']\n",
        "        report += f\"\"\"\n",
        "### Mejor Modelo Base:\n",
        "- **Nombre:** {best_base['model_name']}\n",
        "- **Tipo:** {best_base['model_type']}\n",
        "- **F1-Score:** {best_base['f1_score']:.4f}\n",
        "- **Accuracy:** {best_base['accuracy']:.4f}\n",
        "- **ROC-AUC:** {best_base['roc_auc']:.4f}\n",
        "- **Tiempo de inferencia:** {best_base['inference_time']:.3f}s\n",
        "\"\"\"\n",
        "    \n",
        "    # Resultados de ensembles\n",
        "    if 'best_ensemble' in executive_summary:\n",
        "        best_ensemble = executive_summary['best_ensemble']\n",
        "        report += f\"\"\"\n",
        "### Mejor Ensemble:\n",
        "- **Nombre:** {best_ensemble['ensemble_name']}\n",
        "- **F1-Score:** {best_ensemble['f1_score']:.4f}\n",
        "- **Accuracy:** {best_ensemble['accuracy']:.4f}\n",
        "- **ROC-AUC:** {best_ensemble['roc_auc']:.4f}\n",
        "- **Confianza promedio:** {best_ensemble['avg_confidence']:.3f}\n",
        "- **Muestras alta confianza:** {best_ensemble['high_conf_samples']} ({best_ensemble['high_conf_samples']/executive_summary['dataset_info']['test_samples']*100:.1f}%)\n",
        "- **Accuracy alta confianza:** {best_ensemble['high_conf_accuracy']:.4f}\n",
        "\"\"\"\n",
        "    \n",
        "    # Comparación de rendimiento\n",
        "    if 'performance_comparison' in executive_summary:\n",
        "        comp = executive_summary['performance_comparison']\n",
        "        report += f\"\"\"\n",
        "### Comparación de Rendimiento:\n",
        "- **Mejora absoluta:** {comp['improvement_absolute']:+.4f}\n",
        "- **Mejora porcentual:** {comp['improvement_percentage']:+.2f}%\n",
        "- **Ensemble superior:** {'Sí' if comp['ensemble_superior'] else 'No'}\n",
        "\"\"\"\n",
        "    \n",
        "    report += f\"\"\"\n",
        "\n",
        "## CONCLUSIONES\n",
        "\n",
        "El sistema TruthSeeker ha demostrado {'excelente' if executive_summary.get('best_ensemble', {}).get('f1_score', 0) > 0.8 else 'buen' if executive_summary.get('best_ensemble', {}).get('f1_score', 0) > 0.7 else 'satisfactorio'} rendimiento en la detección de desinformación.\n",
        "\n",
        "### Puntos Destacados:\n",
        "- Sistema ensemble híbrido implementado exitosamente\n",
        "- Múltiples arquitecturas de ML integradas\n",
        "- Sistema de confianza implementado\n",
        "- Visualizaciones interactivas completas generadas\n",
        "- Modelos calibrados y listos para producción\n",
        "\n",
        "### Archivos Generados:\n",
        "- `truthseeker_executive_summary.json`: Resumen ejecutivo completo\n",
        "- `truthseeker_detailed_results.json`: Resultados detallados\n",
        "- `base_models_comparison.csv`: Comparación modelos base\n",
        "- `ensemble_comparison.csv`: Comparación ensembles\n",
        "- `visualizations/`: Dashboards interactivos Plotly\n",
        "- `models/`: Modelos ensemble entrenados\n",
        "\n",
        "### Recomendaciones:\n",
        "1. Usar el mejor ensemble para producción\n",
        "2. Monitorear métricas de confianza en tiempo real\n",
        "3. Considerar re-entrenamiento periódico con nuevos datos\n",
        "4. Implementar A/B testing para validación continua\n",
        "\n",
        "---\n",
        "*Generado por TruthSeeker Ensemble System v{executive_summary['version']}*\n",
        "*{executive_summary['timestamp']}*\n",
        "\"\"\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Guardo resultados finales\n",
        "print(\"Generando reporte final...\")\n",
        "final_summary = save_final_results()\n",
        "final_report = generate_final_report(final_summary)\n",
        "\n",
        "# Guardo reporte\n",
        "with open(RESULTS_PATH / 'truthseeker_final_report.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(final_report)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRUTHSEEKER ENSEMBLE SYSTEM - COMPLETADO\")\n",
        "print(\"=\"*80)\n",
        "print(final_report)\n",
        "print(\"\\nTodos los resultados guardados en:\", RESULTS_PATH)\n",
        "print(\"\\nSistema TruthSeeker listo para detección de desinformación\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
