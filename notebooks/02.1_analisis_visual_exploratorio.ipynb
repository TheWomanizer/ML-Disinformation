{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02.1 Análisis Visual Exploratorio Completo\n",
    "## Sistema Integral Multi-Capa para la Detección de Desinformación\n",
    "\n",
    "**Objetivo**: Realizar un análisis visual exhaustivo de los datos para entender patrones, distribuciones, correlaciones y anomalías a través de múltiples visualizaciones.\n",
    "\n",
    "### Plan de Visualización:\n",
    "1. **Análisis de Distribuciones** - Histogramas, boxplots, violin plots\n",
    "2. **Análisis de Correlaciones** - Heatmaps, scatter plots, pair plots\n",
    "3. **Análisis de Clases** - Balance, distribuciones por clase\n",
    "4. **Detección de Outliers** - Boxplots, scatter plots, análisis Z-score\n",
    "5. **Análisis de Características Lingüísticas** - Patrones textuales\n",
    "6. **Análisis de Redes Sociales** - Métricas de usuarios\n",
    "7. **Análisis Temporal** - Si hay información temporal\n",
    "8. **Análisis Multivariado** - PCA, t-SNE, clustering visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting plotly\n",
      "  Using cached plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in y:\\diplomado\\proyectofinal\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.2-cp313-cp313-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in y:\\diplomado\\proyectofinal\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in y:\\diplomado\\proyectofinal\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "Downloading matplotlib-3.10.6-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.3/8.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.5/8.1 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.5/8.1 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 4.6 MB/s eta 0:00:00\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.2-cp313-cp313-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
      "Downloading narwhals-2.2.0-py3-none-any.whl (401 kB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, pyparsing, pillow, numpy, narwhals, kiwisolver, joblib, fonttools, cycler, scipy, plotly, pandas, contourpy, scikit-learn, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.2 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 narwhals-2.2.0 numpy-2.3.2 pandas-2.3.2 pillow-11.3.0 plotly-6.3.0 pyparsing-3.2.3 pytz-2025.2 scikit-learn-1.7.1 scipy-1.16.1 seaborn-0.13.2 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías de visualización cargadas\n",
      "Configuración de plots establecida\n",
      "Paletas de colores activas\n"
     ]
    }
   ],
   "source": [
    "# Instalar librerías faltantes\n",
    "%pip install pandas numpy matplotlib seaborn plotly scipy scikit-learn\n",
    "\n",
    "# Importaciones esenciales para análisis visual\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew, kurtosis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "# Configuración de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar tamaños de gráficos\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Librerías de visualización cargadas\")\n",
    "print(\"Configuración de plots establecida\")\n",
    "print(\"Paletas de colores activas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset original para análisis visual completo\n",
    "print(\"Cargando dataset original para análisis visual...\")\n",
    "df_original = pd.read_csv('../dataset1/Features_For_Traditional_ML_Techniques.csv')\n",
    "\n",
    "# También cargar datos procesados si existen\n",
    "try:\n",
    "    df_processed = pd.read_csv('../processed_data/dataset_features_processed.csv')\n",
    "    text_data = pd.read_csv('../processed_data/text_data_for_nlp.csv')\n",
    "    print(\"Datos procesados también disponibles\")\n",
    "except:\n",
    "    print(\"Usando solo datos originales\")\n",
    "    df_processed = None\n",
    "    text_data = None\n",
    "\n",
    "print(f\"Dataset original: {df_original.shape[0]:,} filas × {df_original.shape[1]} columnas\")\n",
    "print(f\"Memoria utilizada: {df_original.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Vista rápida\n",
    "print(\"\\nVista previa:\")\n",
    "display(df_original.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis Visual de Distribuciones\n",
    "\n",
    "### 2.1 Distribuciones de Variables Numéricas Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variables numéricas clave para análisis\n",
    "numeric_cols = df_original.select_dtypes(include=[np.number]).columns.tolist()\n",
    "social_media_cols = ['followers_count', 'friends_count', 'favourites_count', 'statuses_count', 'listed_count']\n",
    "engagement_cols = ['mentions', 'quotes', 'replies', 'retweets', 'favourites', 'hashtags', 'URLs']\n",
    "linguistic_cols = ['Word count', 'Max word length', 'Min word length', 'Average word length']\n",
    "bot_cols = ['BotScore', 'BotScoreBinary', 'cred', 'normalize_influence']\n",
    "\n",
    "# Filtrar solo las que existen\n",
    "social_media_cols = [col for col in social_media_cols if col in df_original.columns]\n",
    "engagement_cols = [col for col in engagement_cols if col in df_original.columns]\n",
    "linguistic_cols = [col for col in linguistic_cols if col in df_original.columns]\n",
    "bot_cols = [col for col in bot_cols if col in df_original.columns]\n",
    "\n",
    "print(f\"Métricas de Redes Sociales: {len(social_media_cols)} variables\")\n",
    "print(f\"Métricas de Engagement: {len(engagement_cols)} variables\")\n",
    "print(f\"Métricas Lingüísticas: {len(linguistic_cols)} variables\")\n",
    "print(f\"Métricas de Bot/Credibilidad: {len(bot_cols)} variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 1: Distribuciones de Métricas de Redes Sociales\n",
    "if social_media_cols:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(social_media_cols[:6]):\n",
    "        # Histograma con transformación log para mejor visualización\n",
    "        data = df_original[col].replace(0, 0.1)  # Evitar log(0)\n",
    "        \n",
    "        # Subplot normal\n",
    "        axes[i].hist(data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[i].set_title(f'Distribución: {col}', fontweight='bold', fontsize=12)\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frecuencia')\n",
    "        \n",
    "        # Estadísticas en el gráfico\n",
    "        mean_val = df_original[col].mean()\n",
    "        median_val = df_original[col].median()\n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Media: {mean_val:.0f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Mediana: {median_val:.0f}')\n",
    "        axes[i].legend(fontsize=10)\n",
    "        \n",
    "        # Formato de números grandes\n",
    "        axes[i].ticklabel_format(style='scientific', axis='x', scilimits=(0,0))\n",
    "    \n",
    "    # Ocultar ejes vacíos\n",
    "    for i in range(len(social_media_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Distribuciones de Métricas de Redes Sociales', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 2: Boxplots para detectar outliers en métricas sociales\n",
    "if social_media_cols:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(social_media_cols[:6]):\n",
    "        # Boxplot con escala log para mejor visualización\n",
    "        data = df_original[col].replace(0, 0.1)\n",
    "        \n",
    "        # Crear boxplot\n",
    "        bp = axes[i].boxplot(data, patch_artist=True, vert=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        bp['boxes'][0].set_alpha(0.7)\n",
    "        \n",
    "        axes[i].set_title(f'Outliers en: {col}', fontweight='bold', fontsize=12)\n",
    "        axes[i].set_ylabel(col)\n",
    "        axes[i].set_yscale('log')\n",
    "        \n",
    "        # Estadísticas de outliers\n",
    "        Q1 = df_original[col].quantile(0.25)\n",
    "        Q3 = df_original[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df_original[(df_original[col] < Q1 - 1.5*IQR) | (df_original[col] > Q3 + 1.5*IQR)]\n",
    "        \n",
    "        axes[i].text(0.02, 0.98, f'Outliers: {len(outliers):,}\\n({len(outliers)/len(df_original)*100:.1f}%)', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Ocultar ejes vacíos\n",
    "    for i in range(len(social_media_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Análisis de Outliers - Métricas de Redes Sociales (Escala Log)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Análisis de Engagement y Interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 3: Distribuciones de métricas de engagement\n",
    "if engagement_cols:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(engagement_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(engagement_cols):\n",
    "        if i < len(axes):\n",
    "            # Datos con manejo de ceros\n",
    "            data = df_original[col]\n",
    "            \n",
    "            # Histograma\n",
    "            axes[i].hist(data, bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "            axes[i].set_title(f'{col}', fontweight='bold')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Estadísticas clave\n",
    "            zero_count = (data == 0).sum()\n",
    "            zero_pct = zero_count / len(data) * 100\n",
    "            max_val = data.max()\n",
    "            mean_val = data.mean()\n",
    "            \n",
    "            # Texto informativo\n",
    "            info_text = f'Ceros: {zero_pct:.1f}%\\nMáx: {max_val:,.0f}\\nMedia: {mean_val:.1f}'\n",
    "            axes[i].text(0.95, 0.95, info_text, transform=axes[i].transAxes, \n",
    "                        verticalalignment='top', horizontalalignment='right',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Línea de media\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8)\n",
    "    \n",
    "    # Ocultar ejes vacíos\n",
    "    for i in range(len(engagement_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.suptitle('Distribuciones de Métricas de Engagement', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 4: Violin plots para mostrar distribuciones densas\n",
    "if len(engagement_cols) >= 4:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(engagement_cols[:4]):\n",
    "        # Violin plot con datos transformados para mejor visualización\n",
    "        data = df_original[col].replace(0, 0.01)  # Reemplazar ceros para log\n",
    "        log_data = np.log1p(data)  # log(1+x) para manejar valores pequeños\n",
    "        \n",
    "        parts = axes[i].violinplot([log_data], positions=[0], showmeans=True, showmedians=True)\n",
    "        \n",
    "        # Colorear\n",
    "        for part in parts['bodies']:\n",
    "            part.set_facecolor('lightgreen')\n",
    "            part.set_alpha(0.7)\n",
    "        \n",
    "        axes[i].set_title(f'Densidad: {col} (escala log)', fontweight='bold')\n",
    "        axes[i].set_ylabel(f'log(1 + {col})')\n",
    "        axes[i].set_xticks([])\n",
    "        \n",
    "        # Estadísticas\n",
    "        skewness = skew(data)\n",
    "        kurt = kurtosis(data)\n",
    "        \n",
    "        stats_text = f'Asimetría: {skewness:.2f}\\nCurtosis: {kurt:.2f}'\n",
    "        axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Análisis de Densidad - Métricas de Engagement', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Análisis de Características Lingüísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 5: Características lingüísticas\n",
    "if linguistic_cols:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(linguistic_cols[:4]):\n",
    "        data = df_original[col]\n",
    "        \n",
    "        # Histograma con curva de densidad\n",
    "        axes[i].hist(data, bins=50, alpha=0.7, color='lightcoral', edgecolor='black', density=True)\n",
    "        \n",
    "        # Agregar curva de densidad\n",
    "        if len(data.dropna()) > 0:\n",
    "            try:\n",
    "                from scipy.stats import gaussian_kde\n",
    "                kde = gaussian_kde(data.dropna())\n",
    "                x_range = np.linspace(data.min(), data.max(), 100)\n",
    "                axes[i].plot(x_range, kde(x_range), 'r-', lw=2, label='Densidad')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        axes[i].set_title(f'{col}', fontweight='bold')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Densidad')\n",
    "        \n",
    "        # Estadísticas descriptivas\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        std_val = data.std()\n",
    "        \n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Media: {mean_val:.1f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Mediana: {median_val:.1f}')\n",
    "        \n",
    "        axes[i].legend()\n",
    "        \n",
    "        # Información adicional\n",
    "        info_text = f'Std: {std_val:.1f}\\nMin: {data.min()}\\nMax: {data.max()}'\n",
    "        axes[i].text(0.02, 0.98, info_text, transform=axes[i].transAxes, \n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Ocultar ejes vacíos\n",
    "    for i in range(len(linguistic_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Análisis de Características Lingüísticas', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 6: Matriz de correlación de métricas sociales\n",
    "if len(social_media_cols) > 1:\n",
    "    # Calcular matriz de correlación\n",
    "    corr_social = df_original[social_media_cols].corr()\n",
    "    \n",
    "    # Crear heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_social, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(corr_social, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, mask=mask,\n",
    "                fmt='.2f', annot_kws={'fontsize': 10})\n",
    "    \n",
    "    plt.title('Matriz de Correlación - Métricas de Redes Sociales', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Encontrar las correlaciones más altas\n",
    "    print(\"Top 10 Correlaciones Más Altas:\")\n",
    "    # Obtener pares de correlaciones (excluyendo diagonal)\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_social.columns)):\n",
    "        for j in range(i+1, len(corr_social.columns)):\n",
    "            corr_pairs.append((corr_social.columns[i], corr_social.columns[j], corr_social.iloc[i,j]))\n",
    "    \n",
    "    # Ordenar por correlación absoluta\n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    for i, (var1, var2, corr_val) in enumerate(corr_pairs[:10]):\n",
    "        print(f\"{i+1:2d}. {var1} ↔ {var2}: {corr_val:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 7: Scatter plots de las correlaciones más interesantes\n",
    "if len(social_media_cols) >= 2:\n",
    "    # Seleccionar pares con correlaciones más altas\n",
    "    top_pairs = corr_pairs[:4]  # Top 4 correlaciones\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (var1, var2, corr_val) in enumerate(top_pairs[:4]):\n",
    "        # Scatter plot\n",
    "        x_data = df_original[var1].replace(0, 0.1)\n",
    "        y_data = df_original[var2].replace(0, 0.1)\n",
    "        \n",
    "        axes[i].scatter(x_data, y_data, alpha=0.3, s=20, color='blue')\n",
    "        \n",
    "        # Línea de tendencia\n",
    "        try:\n",
    "            z = np.polyfit(x_data, y_data, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[i].plot(x_data, p(x_data), \"r--\", alpha=0.8, linewidth=2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        axes[i].set_xlabel(var1)\n",
    "        axes[i].set_ylabel(var2)\n",
    "        axes[i].set_title(f'{var1} vs {var2}\\nCorrelación: {corr_val:+.3f}', fontweight='bold')\n",
    "        \n",
    "        # Usar escala log si hay valores muy dispersos\n",
    "        if x_data.max() / x_data.min() > 1000:\n",
    "            axes[i].set_xscale('log')\n",
    "        if y_data.max() / y_data.min() > 1000:\n",
    "            axes[i].set_yscale('log')\n",
    "        \n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Análisis de Correlaciones - Scatter Plots', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de Balance de Clases y Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 8: Análisis completo de la variable objetivo\n",
    "target_col = 'BinaryNumTarget'\n",
    "\n",
    "if target_col in df_original.columns:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Distribución de clases - Bar plot\n",
    "    class_counts = df_original[target_col].value_counts().sort_index()\n",
    "    colors = ['lightcoral', 'skyblue']\n",
    "    bars = axes[0,0].bar([f'Clase {int(k)}' for k in class_counts.index], class_counts.values, color=colors)\n",
    "    axes[0,0].set_title('Distribución de Clases', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Número de Muestras')\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for bar, count in zip(bars, class_counts.values):\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                      f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Pie chart\n",
    "    labels = [f'Clase {int(k)}\\n({count:,})' for k, count in class_counts.items()]\n",
    "    wedges, texts, autotexts = axes[0,1].pie(class_counts.values, labels=labels, autopct='%1.2f%%', \n",
    "                                            colors=colors, startangle=90, explode=(0.05, 0))\n",
    "    axes[0,1].set_title('Proporción de Clases', fontweight='bold')\n",
    "    \n",
    "    # 3. Análisis de desbalance\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    axes[0,2].text(0.5, 0.7, f'Ratio de Desbalance', fontsize=16, fontweight='bold', \n",
    "                  ha='center', transform=axes[0,2].transAxes)\n",
    "    axes[0,2].text(0.5, 0.5, f'{imbalance_ratio:.2f}:1', fontsize=24, fontweight='bold', \n",
    "                  ha='center', transform=axes[0,2].transAxes, color='red')\n",
    "    \n",
    "    if imbalance_ratio > 5:\n",
    "        severity = \"SEVERO\"\n",
    "        color = 'red'\n",
    "    elif imbalance_ratio > 3:\n",
    "        severity = \"MODERADO\"\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        severity = \"LEVE\"\n",
    "        color = 'green'\n",
    "    \n",
    "    axes[0,2].text(0.5, 0.3, f'Desbalance: {severity}', fontsize=14, fontweight='bold', \n",
    "                  ha='center', transform=axes[0,2].transAxes, color=color)\n",
    "    axes[0,2].axis('off')\n",
    "    \n",
    "    # 4. Distribución de una métrica por clase (ejemplo: followers_count)\n",
    "    if 'followers_count' in df_original.columns:\n",
    "        for class_val in class_counts.index:\n",
    "            class_data = df_original[df_original[target_col] == class_val]['followers_count']\n",
    "            class_data_log = np.log1p(class_data.replace(0, 0.1))\n",
    "            axes[1,0].hist(class_data_log, bins=30, alpha=0.6, \n",
    "                          label=f'Clase {int(class_val)} (n={len(class_data):,})',\n",
    "                          color=colors[int(class_val)])\n",
    "        \n",
    "        axes[1,0].set_title('Distribución de Followers por Clase', fontweight='bold')\n",
    "        axes[1,0].set_xlabel('log(1 + followers_count)')\n",
    "        axes[1,0].set_ylabel('Frecuencia')\n",
    "        axes[1,0].legend()\n",
    "    \n",
    "    # 5. Box plot de otra métrica por clase\n",
    "    if 'BotScore' in df_original.columns:\n",
    "        bot_by_class = [df_original[df_original[target_col] == class_val]['BotScore'] \n",
    "                       for class_val in class_counts.index]\n",
    "        \n",
    "        bp = axes[1,1].boxplot(bot_by_class, labels=[f'Clase {int(k)}' for k in class_counts.index], \n",
    "                              patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        axes[1,1].set_title('BotScore por Clase', fontweight='bold')\n",
    "        axes[1,1].set_ylabel('BotScore')\n",
    "    \n",
    "    # 6. Estadísticas por clase\n",
    "    stats_text = \"Estadísticas por Clase:\\n\\n\"\n",
    "    for class_val in class_counts.index:\n",
    "        class_data = df_original[df_original[target_col] == class_val]\n",
    "        stats_text += f\"Clase {int(class_val)}:\\n\"\n",
    "        stats_text += f\"  N: {len(class_data):,}\\n\"\n",
    "        stats_text += f\"  %: {len(class_data)/len(df_original)*100:.1f}%\\n\\n\"\n",
    "    \n",
    "    axes[1,2].text(0.05, 0.95, stats_text, transform=axes[1,2].transAxes, \n",
    "                  verticalalignment='top', fontsize=12,\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Análisis Completo de Variable Objetivo', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\" Resumen del análisis de clases:\")\n",
    "    print(f\"   - Total de muestras: {len(df_original):,}\")\n",
    "    print(f\"   - Clases encontradas: {len(class_counts)}\")\n",
    "    print(f\"   - Ratio de desbalance: {imbalance_ratio:.2f}:1\")\n",
    "    print(f\"   - Nivel de desbalance: {severity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de Outliers Avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 9: Detección de outliers multivariado\n",
    "# Seleccionar variables numéricas clave para análisis de outliers\n",
    "key_vars = ['followers_count', 'friends_count', 'favourites_count', 'BotScore']\n",
    "available_vars = [var for var in key_vars if var in df_original.columns]\n",
    "\n",
    "if len(available_vars) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # 1. Z-score analysis\n",
    "    z_scores_data = []\n",
    "    for var in available_vars[:4]:\n",
    "        z_scores = np.abs(zscore(df_original[var], nan_policy='omit'))\n",
    "        z_scores_data.append(z_scores)\n",
    "    \n",
    "    axes[0].boxplot(z_scores_data, labels=available_vars[:4])\n",
    "    axes[0].axhline(y=3, color='red', linestyle='--', alpha=0.8, label='Umbral 3σ')\n",
    "    axes[0].axhline(y=2, color='orange', linestyle='--', alpha=0.8, label='Umbral 2σ')\n",
    "    axes[0].set_title('Z-Scores por Variable', fontweight='bold')\n",
    "    axes[0].set_ylabel('|Z-Score|')\n",
    "    axes[0].legend()\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Scatter plot con outliers marcados\n",
    "    if len(available_vars) >= 2:\n",
    "        x_var, y_var = available_vars[0], available_vars[1]\n",
    "        x_data = df_original[x_var]\n",
    "        y_data = df_original[y_var]\n",
    "        \n",
    "        # Calcular outliers\n",
    "        x_z = np.abs(zscore(x_data, nan_policy='omit'))\n",
    "        y_z = np.abs(zscore(y_data, nan_policy='omit'))\n",
    "        \n",
    "        # Clasificar puntos\n",
    "        normal = (x_z < 2) & (y_z < 2)\n",
    "        moderate_outliers = ((x_z >= 2) & (x_z < 3)) | ((y_z >= 2) & (y_z < 3))\n",
    "        extreme_outliers = (x_z >= 3) | (y_z >= 3)\n",
    "        \n",
    "        # Plot\n",
    "        axes[1].scatter(x_data[normal], y_data[normal], alpha=0.6, s=20, \n",
    "                       color='blue', label=f'Normal (n={normal.sum():,})')\n",
    "        axes[1].scatter(x_data[moderate_outliers], y_data[moderate_outliers], \n",
    "                       alpha=0.8, s=30, color='orange', \n",
    "                       label=f'Outliers Moderados (n={moderate_outliers.sum():,})')\n",
    "        axes[1].scatter(x_data[extreme_outliers], y_data[extreme_outliers], \n",
    "                       alpha=1.0, s=40, color='red', \n",
    "                       label=f'Outliers Extremos (n={extreme_outliers.sum():,})')\n",
    "        \n",
    "        axes[1].set_xlabel(x_var)\n",
    "        axes[1].set_ylabel(y_var)\n",
    "        axes[1].set_title(f'Outliers: {x_var} vs {y_var}', fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].set_xscale('log')\n",
    "        axes[1].set_yscale('log')\n",
    "    \n",
    "    # 3. Distribución de máximos Z-scores por muestra\n",
    "    max_z_scores = np.max(z_scores_data, axis=0)\n",
    "    axes[2].hist(max_z_scores, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[2].axvline(x=2, color='orange', linestyle='--', alpha=0.8, label='Umbral 2σ')\n",
    "    axes[2].axvline(x=3, color='red', linestyle='--', alpha=0.8, label='Umbral 3σ')\n",
    "    axes[2].set_title('Distribución de Z-Score Máximo por Muestra', fontweight='bold')\n",
    "    axes[2].set_xlabel('Máximo |Z-Score|')\n",
    "    axes[2].set_ylabel('Frecuencia')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    # 4. Resumen estadístico de outliers\n",
    "    outlier_summary = []\n",
    "    for var in available_vars:\n",
    "        z_scores = np.abs(zscore(df_original[var], nan_policy='omit'))\n",
    "        moderate = ((z_scores >= 2) & (z_scores < 3)).sum()\n",
    "        extreme = (z_scores >= 3).sum()\n",
    "        outlier_summary.append({\n",
    "            'Variable': var,\n",
    "            'Moderados_2σ': moderate,\n",
    "            'Extremos_3σ': extreme,\n",
    "            'Total_Outliers': moderate + extreme,\n",
    "            'Pct_Outliers': (moderate + extreme) / len(df_original) * 100\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    \n",
    "    # Tabla como texto\n",
    "    table_text = \"Resumen de Outliers por Variable:\\n\\n\"\n",
    "    table_text += f\"{'Variable':<15} {'2σ':<6} {'3σ':<6} {'Total':<6} {'%':<6}\\n\"\n",
    "    table_text += \"-\"*45 + \"\\n\"\n",
    "    for _, row in outlier_df.iterrows():\n",
    "        table_text += f\"{row['Variable']:<15} {row['Moderados_2σ']:<6} {row['Extremos_3σ']:<6} {row['Total_Outliers']:<6} {row['Pct_Outliers']:<6.1f}\\n\"\n",
    "    \n",
    "    axes[3].text(0.05, 0.95, table_text, transform=axes[3].transAxes, \n",
    "                verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Análisis Avanzado de Outliers', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nResumen de outliers:\")\n",
    "    total_extreme = outlier_df['Extremos_3σ'].sum()\n",
    "    total_moderate = outlier_df['Moderados_2σ'].sum()\n",
    "    print(f\"   - Outliers extremos (>3σ): {total_extreme:,}\")\n",
    "    print(f\"   - Outliers moderados (2-3σ): {total_moderate:,}\")\n",
    "    print(f\"   - Porcentaje total de muestras con outliers: {(total_extreme + total_moderate)/len(df_original)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Entidades y Características NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 10: Análisis de entidades nombradas\n",
    "entity_cols = [col for col in df_original.columns if '_percentage' in col]\n",
    "\n",
    "if entity_cols:\n",
    "    print(f\"Encontradas {len(entity_cols)} tipos de entidades nombradas\")\n",
    "    \n",
    "    # Seleccionar las más relevantes para visualización\n",
    "    top_entity_cols = entity_cols[:12]  # Top 12 para visualizar\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    entity_stats = []\n",
    "    \n",
    "    for i, col in enumerate(top_entity_cols):\n",
    "        data = df_original[col]\n",
    "        \n",
    "        # Histograma\n",
    "        axes[i].hist(data, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[i].set_title(f'{col.replace(\"_percentage\", \"\")}', fontweight='bold', fontsize=10)\n",
    "        axes[i].set_xlabel('Porcentaje')\n",
    "        axes[i].set_ylabel('Frecuencia')\n",
    "        \n",
    "        # Estadísticas\n",
    "        zero_pct = (data == 0).sum() / len(data) * 100\n",
    "        mean_val = data.mean()\n",
    "        max_val = data.max()\n",
    "        \n",
    "        # Texto informativo\n",
    "        info_text = f'Ceros: {zero_pct:.1f}%\\nMedia: {mean_val:.3f}\\nMáx: {max_val:.3f}'\n",
    "        axes[i].text(0.95, 0.95, info_text, transform=axes[i].transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right', fontsize=8,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Guardar estadísticas\n",
    "        entity_stats.append({\n",
    "            'Entidad': col.replace('_percentage', ''),\n",
    "            'Media': mean_val,\n",
    "            'Ceros_%': zero_pct,\n",
    "            'Max': max_val,\n",
    "            'Presencia_%': 100 - zero_pct\n",
    "        })\n",
    "        \n",
    "        if mean_val > 0:\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8)\n",
    "    \n",
    "    plt.suptitle('Distribuciones de Entidades Nombradas (Named Entities)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Crear resumen de entidades más presentes\n",
    "    entity_df = pd.DataFrame(entity_stats)\n",
    "    entity_df = entity_df.sort_values('Presencia_%', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Entidades Más Presentes:\")\n",
    "    for i, (_, row) in enumerate(entity_df.head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}. {row['Entidad']:<15}: {row['Presencia_%']:5.1f}% presencia, media={row['Media']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 11: Características sintácticas (POS tags)\n",
    "pos_cols = ['present_verbs', 'past_verbs', 'adjectives', 'adverbs', 'adpositions', \n",
    "           'pronouns', 'TOs', 'determiners', 'conjunctions']\n",
    "available_pos_cols = [col for col in pos_cols if col in df_original.columns]\n",
    "\n",
    "if available_pos_cols:\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    pos_stats = []\n",
    "    \n",
    "    for i, col in enumerate(available_pos_cols[:9]):\n",
    "        data = df_original[col]\n",
    "        \n",
    "        # Histograma con curva de densidad\n",
    "        axes[i].hist(data, bins=30, alpha=0.7, color='lightblue', edgecolor='black', density=True)\n",
    "        \n",
    "        # Estadísticas\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        std_val = data.std()\n",
    "        \n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Media: {mean_val:.1f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Mediana: {median_val:.1f}')\n",
    "        \n",
    "        axes[i].set_title(col.replace('_', ' ').title(), fontweight='bold')\n",
    "        axes[i].set_xlabel('Frecuencia')\n",
    "        axes[i].set_ylabel('Densidad')\n",
    "        axes[i].legend(fontsize=8)\n",
    "        \n",
    "        # Guardar estadísticas\n",
    "        pos_stats.append({\n",
    "            'POS_Tag': col,\n",
    "            'Media': mean_val,\n",
    "            'Mediana': median_val,\n",
    "            'Std': std_val,\n",
    "            'CV': std_val/mean_val if mean_val > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Ocultar ejes vacíos\n",
    "    for i in range(len(available_pos_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Distribuciones de Características Sintácticas (POS Tags)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Crear heatmap de correlación entre POS tags\n",
    "    if len(available_pos_cols) > 1:\n",
    "        pos_corr = df_original[available_pos_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(pos_corr, dtype=bool))\n",
    "        sns.heatmap(pos_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
    "                    square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, mask=mask,\n",
    "                    fmt='.2f', annot_kws={'fontsize': 9})\n",
    "        plt.title('Correlaciones entre Características Sintácticas', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis Multivariado y Reducción Dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 12: PCA para análisis de componentes principales\n",
    "# Preparar datos para PCA (solo variables numéricas, excluyendo target)\n",
    "numeric_features = df_original.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'BinaryNumTarget' in numeric_features:\n",
    "    numeric_features.remove('BinaryNumTarget')\n",
    "if 'Unnamed: 0' in numeric_features:\n",
    "    numeric_features.remove('Unnamed: 0')\n",
    "\n",
    "# Tomar una muestra para análisis más rápido\n",
    "sample_size = min(10000, len(df_original))\n",
    "df_sample = df_original.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Preparar datos para PCA\n",
    "X = df_sample[numeric_features].fillna(0)\n",
    "y = df_sample['BinaryNumTarget'] if 'BinaryNumTarget' in df_sample.columns else None\n",
    "\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Crear visualizaciones de PCA\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Varianza explicada acumulativa\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "axes[0,0].plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-', markersize=4)\n",
    "axes[0,0].axhline(y=0.8, color='red', linestyle='--', alpha=0.8, label='80% Varianza')\n",
    "axes[0,0].axhline(y=0.95, color='orange', linestyle='--', alpha=0.8, label='95% Varianza')\n",
    "axes[0,0].set_title('Varianza Explicada Acumulativa - PCA', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Número de Componentes')\n",
    "axes[0,0].set_ylabel('Varianza Explicada Acumulativa')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Varianza por componente individual\n",
    "axes[0,1].bar(range(1, 21), pca.explained_variance_ratio_[:20], alpha=0.7, color='lightcoral')\n",
    "axes[0,1].set_title('Varianza por Componente Individual (Top 20)', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Componente Principal')\n",
    "axes[0,1].set_ylabel('Proporción de Varianza Explicada')\n",
    "\n",
    "# 3. Scatter plot de primeras dos componentes principales\n",
    "if y is not None:\n",
    "    # Colorear por clase\n",
    "    scatter = axes[1,0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.6, s=20, cmap='coolwarm')\n",
    "    axes[1,0].set_title('PCA: PC1 vs PC2 (coloreado por clase)', fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=axes[1,0])\n",
    "else:\n",
    "    axes[1,0].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, s=20, color='blue')\n",
    "    axes[1,0].set_title('PCA: PC1 vs PC2', fontweight='bold')\n",
    "\n",
    "axes[1,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "axes[1,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Top features en PC1 y PC2\n",
    "# Obtener las características más importantes en PC1\n",
    "pc1_features = np.abs(pca.components_[0])\n",
    "pc1_top_idx = np.argsort(pc1_features)[-10:][::-1]\n",
    "pc1_top_features = [numeric_features[i] for i in pc1_top_idx]\n",
    "pc1_top_values = pc1_features[pc1_top_idx]\n",
    "\n",
    "axes[1,1].barh(range(10), pc1_top_values, color='lightblue')\n",
    "axes[1,1].set_yticks(range(10))\n",
    "axes[1,1].set_yticklabels([f.replace('_', ' ')[:20] for f in pc1_top_features], fontsize=8)\n",
    "axes[1,1].set_title('Top 10 Features en PC1', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Contribución Absoluta')\n",
    "\n",
    "plt.suptitle('Análisis de Componentes Principales (PCA)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir estadísticas de PCA\n",
    "components_80 = np.argmax(cumsum_var >= 0.8) + 1\n",
    "components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nResumen PCA:\")\n",
    "print(f\"   - Componentes para 80% varianza: {components_80}\")\n",
    "print(f\"   - Componentes para 95% varianza: {components_95}\")\n",
    "print(f\"   - Varianza PC1: {pca.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"   - Varianza PC2: {pca.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"   - Varianza PC1+PC2: {pca.explained_variance_ratio_[:2].sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análisis de Texto (si están disponibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 13: Análisis básico de texto (si está disponible)\n",
    "if text_data is not None and not text_data.empty:\n",
    "    print(\"Analizando datos de texto...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, text_col in enumerate(['statement', 'tweet']):\n",
    "        if text_col in text_data.columns and i < len(axes):\n",
    "            # Longitud de textos\n",
    "            text_lengths = text_data[text_col].fillna('').str.len()\n",
    "            \n",
    "            axes[i*3].hist(text_lengths, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "            axes[i*3].set_title(f'Longitud de {text_col}', fontweight='bold')\n",
    "            axes[i*3].set_xlabel('Caracteres')\n",
    "            axes[i*3].set_ylabel('Frecuencia')\n",
    "            axes[i*3].axvline(text_lengths.mean(), color='red', linestyle='--', \n",
    "                             label=f'Media: {text_lengths.mean():.0f}')\n",
    "            axes[i*3].legend()\n",
    "            \n",
    "            # Número de palabras\n",
    "            word_counts = text_data[text_col].fillna('').str.split().str.len()\n",
    "            \n",
    "            axes[i*3+1].hist(word_counts, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "            axes[i*3+1].set_title(f'Número de palabras - {text_col}', fontweight='bold')\n",
    "            axes[i*3+1].set_xlabel('Palabras')\n",
    "            axes[i*3+1].set_ylabel('Frecuencia')\n",
    "            axes[i*3+1].axvline(word_counts.mean(), color='red', linestyle='--', \n",
    "                               label=f'Media: {word_counts.mean():.1f}')\n",
    "            axes[i*3+1].legend()\n",
    "            \n",
    "            # Estadísticas de texto\n",
    "            stats_text = f\"Estadísticas de {text_col}:\\n\\n\"\n",
    "            stats_text += f\"Total textos: {len(text_data):,}\\n\"\n",
    "            stats_text += f\"Textos vacíos: {text_data[text_col].isna().sum():,}\\n\\n\"\n",
    "            stats_text += f\"Longitud (chars):\\n\"\n",
    "            stats_text += f\"  Media: {text_lengths.mean():.1f}\\n\"\n",
    "            stats_text += f\"  Mediana: {text_lengths.median():.1f}\\n\"\n",
    "            stats_text += f\"  Min: {text_lengths.min()}\\n\"\n",
    "            stats_text += f\"  Max: {text_lengths.max()}\\n\\n\"\n",
    "            stats_text += f\"Palabras:\\n\"\n",
    "            stats_text += f\"  Media: {word_counts.mean():.1f}\\n\"\n",
    "            stats_text += f\"  Mediana: {word_counts.median():.1f}\\n\"\n",
    "            stats_text += f\"  Min: {word_counts.min()}\\n\"\n",
    "            stats_text += f\"  Max: {word_counts.max()}\\n\"\n",
    "            \n",
    "            axes[i*3+2].text(0.05, 0.95, stats_text, transform=axes[i*3+2].transAxes, \n",
    "                            verticalalignment='top', fontfamily='monospace', fontsize=9,\n",
    "                            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "            axes[i*3+2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Análisis de Características de Texto', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Datos de texto no disponibles para análisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen y Conclusiones del Análisis Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS 14: Dashboard de resumen\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMEN EJECUTIVO DEL ANÁLISIS VISUAL EXPLORATORIO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Estadísticas generales\n",
    "print(f\"\\nESTADÍSTICAS GENERALES:\")\n",
    "print(f\"   - Total de muestras: {len(df_original):,}\")\n",
    "print(f\"   - Total de características: {len(df_original.columns)}\")\n",
    "print(f\"   - Características numéricas: {len(numeric_features)}\")\n",
    "print(f\"   - Memoria utilizada: {df_original.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Balance de clases\n",
    "if 'BinaryNumTarget' in df_original.columns:\n",
    "    class_counts = df_original['BinaryNumTarget'].value_counts().sort_index()\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    print(f\"\\nBALANCE DE CLASES:\")\n",
    "    for clase in class_counts.index:\n",
    "        count = class_counts[clase]\n",
    "        prop = count / len(df_original) * 100\n",
    "        label = \"Falso\" if clase == 0 else \"Verdadero\"\n",
    "        print(f\"   • Clase {clase} ({label}): {count:,} ({prop:.1f}%)\")\n",
    "    print(f\"   • Ratio de desbalance: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# 3. Outliers principales\n",
    "print(f\"\\nOUTLIERS DETECTADOS:\")\n",
    "key_vars = ['followers_count', 'friends_count', 'favourites_count']\n",
    "available_vars = [var for var in key_vars if var in df_original.columns]\n",
    "\n",
    "total_extreme_outliers = 0\n",
    "for var in available_vars:\n",
    "    z_scores = np.abs(zscore(df_original[var], nan_policy='omit'))\n",
    "    extreme = (z_scores >= 3).sum()\n",
    "    total_extreme_outliers += extreme\n",
    "    print(f\"   - {var}: {extreme:,} outliers extremos (>3σ)\")\n",
    "\n",
    "print(f\"   - Total estimado de muestras con outliers extremos: {total_extreme_outliers:,}\")\n",
    "\n",
    "# 4. Características más variables\n",
    "print(f\"\\nCARACTERÍSTICAS MÁS VARIABLES:\")\n",
    "cv_stats = []\n",
    "for col in numeric_features[:20]:  # Top 20 para no sobrecargar\n",
    "    if col in df_original.columns:\n",
    "        data = df_original[col]\n",
    "        cv = data.std() / data.mean() if data.mean() != 0 else 0\n",
    "        cv_stats.append((col, cv))\n",
    "\n",
    "cv_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (col, cv) in enumerate(cv_stats[:10]):\n",
    "    print(f\"   {i+1:2d}. {col:<25}: CV = {cv:.2f}\")\n",
    "\n",
    "# 5. Entidades más presentes\n",
    "entity_cols = [col for col in df_original.columns if '_percentage' in col]\n",
    "if entity_cols:\n",
    "    print(f\"\\nENTIDADES MÁS PRESENTES:\")\n",
    "    entity_presence = []\n",
    "    for col in entity_cols:\n",
    "        presence = (df_original[col] > 0).sum() / len(df_original) * 100\n",
    "        entity_presence.append((col.replace('_percentage', ''), presence))\n",
    "    \n",
    "    entity_presence.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (entity, presence) in enumerate(entity_presence[:10]):\n",
    "        print(f\"   {i+1:2d}. {entity:<15}: {presence:5.1f}% de textos\")\n",
    "\n",
    "# 6. Recomendaciones\n",
    "print(f\"\\nRECOMENDACIONES PARA MODELADO:\")\n",
    "print(f\"   - Aplicar técnicas de manejo de outliers (RobustScaler recomendado)\")\n",
    "print(f\"   - Considerar transformaciones log para variables muy asimétricas\")\n",
    "print(f\"   - Usar validación estratificada debido al desbalance de clases\")\n",
    "if imbalance_ratio > 3:\n",
    "    print(f\"   - Implementar técnicas de balanceamiento (SMOTE, class_weight)\")\n",
    "print(f\"   - Considerar selección de características (muchas features disponibles)\")\n",
    "print(f\"   - Evaluar con métricas balanceadas (F1-score, precision, recall)\")\n",
    "\n",
    "print(f\"\\nANÁLISIS VISUAL EXPLORATORIO COMPLETADO\")\n",
    "print(f\"Se generaron {14} análisis visuales comprehensivos\")\n",
    "print(f\"Datos listos para fase de modelado\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
