# Gu√≠a Maestra del Proyecto: Sistema Integral Multi-Capa para la Detecci√≥n de Desinformaci√≥n

*Versi√≥n 2.0 - Edici√≥n Detallada*

## 1. Filosof√≠a y Objetivos del Proyecto

Este documento es la gu√≠a maestra para la construcci√≥n de un sistema avanzado de detecci√≥n de desinformaci√≥n. El enfoque va m√°s all√° de una simple clasificaci√≥n binaria. Buscamos crear un sistema hol√≠stico que responda a tres preguntas clave:

1.  **¬øQU√â?** -> ¬øEs esta noticia falsa o verdadera? (Clasificaci√≥n de Contenido)
2.  **¬øQUI√âN?** -> ¬øLa fuente que publica la noticia es cre√≠ble? ¬øEs un bot? (An√°lisis de la Fuente)
3.  **¬øPOR QU√â?** -> ¬øQu√© factores (palabras clave, caracter√≠sticas del autor) llevaron al sistema a su conclusi√≥n? (Explicabilidad)

El objetivo final es desarrollar un prototipo de sistema robusto, interpretable y multi-capa, utilizando el `dataset1` como n√∫cleo y el `dataset2` para un an√°lisis contextual final.

## 2. Configuraci√≥n del Entorno de Desarrollo

(Sin cambios, la secci√≥n anterior era correcta)

## 3. Fases de Ejecuci√≥n: Gu√≠a Paso a Paso

---

### **Fase 0: El Fundamento - Limpieza y Preparaci√≥n de Datos**

*   **üéØ Objetivo:** Transformar los datos crudos en conjuntos de datos limpios, validados y listos para el modelado. El 90% del √©xito de un proyecto de ML reside en esta fase.

#### **Actividad 0.1: Preparaci√≥n de `dataset1`**

*   **üìù Plan de Acci√≥n Detallado:**

    1.  **An√°lisis de Nulos y Ceros (Validaci√≥n Sem√°ntica):**
        *   **Justificaci√≥n:** No podemos confiar ciegamente en los datos. Un valor de 0 o `NaN` puede significar cosas distintas. Debemos entenderlo antes de actuar.
        *   **C√≥mo:** Usar `df.info()`, `df.isnull().sum()`, y `df.describe().transpose()` para obtener un panorama general. Luego, generar histogramas (`df.hist()`) para visualizar la distribuci√≥n de cada variable y detectar picos an√≥malos en cero.
        *   **Conclusi√≥n Esperada:** Para este dataset, la mayor√≠a de los ceros son **significativos** (ej. 0 adjetivos en un texto, 0% de entidades de un tipo). No los trataremos como nulos, pero este paso de validaci√≥n es una pr√°ctica indispensable.

    2.  **An√°lisis de Desbalance de Clases (Paso Cr√≠tico):**
        *   **Justificaci√≥n:** Si tenemos un 95% de noticias verdaderas y un 5% de falsas, un modelo ingenuo que siempre prediga "Verdadero" tendr√° un 95% de accuracy, pero ser√° in√∫til. Debemos saber si nuestras clases est√°n desbalanceadas para elegir las m√©tricas y t√©cnicas de modelado correctas.
        *   **C√≥mo:** `print(df1['BinaryNumTarget'].value_counts(normalize=True))`. 
        *   **Acci√≥n:** Si hay desbalance, usaremos `stratify` en `train_test_split` y m√©tricas como el **F1-Score** en lugar de la exactitud (accuracy).

    3.  **Limpieza y Selecci√≥n de Columnas:**
        *   **Justificaci√≥n:** Evitar el *data leakage* y eliminar datos no relevantes para la tarea de modelado cl√°sica.
        *   **C√≥mo:**
            ```python
            # Data Leakage: 'majority_target' es una versi√≥n del target. Eliminarla es obligatorio.
            df1.drop(columns=['majority_target'], inplace=True)

            # Columnas de texto: No se usan en el modelo cl√°sico. Se guardan para la fase de NLP.
            text_data = df1[['statement', 'tweet']]
            df1.drop(columns=['statement', 'tweet'], inplace=True)
            ```

    4.  **Imputaci√≥n de Valores Faltantes (NaNs):**
        *   **Justificaci√≥n:** Los algoritmos de ML no pueden manejar valores `NaN`. Necesitamos una estrategia para rellenarlos.
        *   **C√≥mo:** Usar la **mediana** es preferible a la media, ya que no se ve afectada por valores extremos (outliers).
            ```python
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='median')
            # Aplicar solo a las columnas que lo necesiten
            df1['followers_count'] = imputer.fit_transform(df1[['followers_count']]) 
            ```

    5.  **Escalado de Caracter√≠sticas Num√©ricas:**
        *   **Justificaci√≥n:** Algoritmos como SVM o Regresi√≥n Log√≠stica son sensibles a la escala de las variables. Estandarizar los datos ayuda a que el entrenamiento sea m√°s r√°pido y eficiente.
        *   **C√≥mo:** `StandardScaler` es la opci√≥n est√°ndar. Transforma los datos para que tengan una media de 0 y desviaci√≥n est√°ndar de 1.
            ```python
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            numeric_cols = df1.select_dtypes(include=np.number).columns.drop('BinaryNumTarget')
            df1[numeric_cols] = scaler.fit_transform(df1[numeric_cols])
            ```

*   **‚úÖ Entregable:** Un DataFrame `df1_processed` listo para el modelado.

---

### **Fase 1: El Motor de Clasificaci√≥n de Contenido**

*   **üéØ Objetivo:** Construir y evaluar nuestros dos modelos predictivos principales.

#### **Actividad 1.1: Modelo con Features Tradicionales (XGBoost)**

*   **üìù Pasos:**
    1.  **Divisi√≥n Estratificada:** Dividir `df1_processed` en entrenamiento y prueba. Usar `stratify=y` es crucial si hay desbalance de clases para mantener la misma proporci√≥n en ambos conjuntos.
        ```python
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        ```
    2.  **Entrenamiento del Modelo:** Entrenar `XGBClassifier`. Si hay desbalance, se puede usar el par√°metro `scale_pos_weight` para dar m√°s importancia a la clase minoritaria.
    3.  **Evaluaci√≥n Rigurosa:** Usar `classification_report` (que incluye F1-score) y la matriz de confusi√≥n para entender el rendimiento en cada clase.

#### **Actividad 1.2: Modelo con NLP (Transformer - BERT)**

*   **Justificaci√≥n:** Mientras que XGBoost ve los datos como una bolsa de n√∫meros, BERT lee y entiende el texto, capturando sarcasmo, contexto y relaciones sem√°nticas. Es un enfoque mucho m√°s profundo.
*   **Consejo Profesional:** Fine-tuning de BERT es computacionalmente intensivo. Es recomendable empezar con una muestra peque√±a del dataset (ej. 1000 filas) para asegurar que todo el pipeline funciona, antes de lanzarlo sobre el conjunto de datos completo, preferiblemente en un entorno con GPU (como Google Colab).
*   **üìù Pasos:**
    1.  **Carga y Preparaci√≥n:** Usar la librer√≠a `datasets` de Hugging Face. Cargar un modelo pre-entrenado **en espa√±ol** es clave (`dccuchile/bert-base-spanish-wwm-uncased` es una excelente opci√≥n).
    2.  **Fine-tuning:** Seguir el proceso de tokenizaci√≥n y entrenamiento. La librer√≠a `Trainer` de Hugging Face simplifica mucho este proceso y maneja la optimizaci√≥n por nosotros.

---

### **Fase 2: El M√≥dulo de An√°lisis de la Fuente**

*   **üéØ Objetivo:** Ir m√°s all√° del contenido y evaluar al mensajero.

*   **üìù Pasos:**
    1.  **Puntuaci√≥n de Credibilidad (Target Encoding con Precauci√≥n):**
        *   **Justificaci√≥n:** La idea de reemplazar a un autor por su ratio hist√≥rico de veracidad es potente. Esto se llama *Target Encoding*.
        *   **Advertencia:** Hacer esto ingenuamente puede causar *overfitting*. Un m√©todo m√°s seguro es calcular estos scores usando solo el conjunto de entrenamiento y luego aplicarlos al de prueba, o usar una estrategia de validaci√≥n cruzada.
    2.  **Detector de Bots:** Entrenar un clasificador para predecir `BotScoreBinary`.

---

### **Fase 3, 4 y 5 (Sinopsis Mejorada)**

*   **Fase 3 (Modelo H√≠brido):**
    *   **Justificaci√≥n:** Unimos los mundos: el an√°lisis num√©rico de XGBoost, la comprensi√≥n sem√°ntica de BERT y la credibilidad de la fuente. El meta-modelo aprende a ponderar la opini√≥n de cada experto para tomar la mejor decisi√≥n.
*   **Fase 4 (Explicabilidad - XAI):**
    *   **Justificaci√≥n:** Un modelo que dice "Falso" sin m√°s, es una caja negra. Un modelo que dice "Falso PORQUE el autor tiene muy baja credibilidad Y el texto usa un lenguaje emocionalmente cargado" es una herramienta de an√°lisis.
*   **Fase 5 (Estudio Contextual):**
    *   **Justificaci√≥n:** Este es el paso final de la ciencia de datos: la s√≠ntesis. Conectamos nuestros hallazgos con un problema m√°s amplio (comportamiento en redes), generando hip√≥tesis que podr√≠an guiar futuras investigaciones.

## 4. Estructura de Carpetas Sugerida

(Sin cambios, la estructura propuesta es robusta)

Este documento es ahora un plan de batalla mucho m√°s completo y profesional. Refleja el proceso de pensamiento iterativo y cuidadoso que requiere un proyecto de esta magnitud.